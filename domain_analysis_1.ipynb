{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b861b53-1f5b-4b02-a80a-fa3deeac06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from geopy.geocoders import GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderQuotaExceeded\n",
    "import time\n",
    "import re\n",
    "from geopy.geocoders import GoogleV3\n",
    "import time\n",
    "import requests\n",
    "from fuzzywuzzy import process\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4b2e68-5ad4-4860-804a-d73c81f1a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/Users/gayathri/Documents/GPE_intra_domain_cleaned/'\n",
    "final_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "\n",
    "api_key = 'AIzaSyBJ8P42fvaYv5pmqNdEqYEOJPENnm7eND0'\n",
    "geolocator = GoogleV3(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b915b447-9e66-4569-b02d-8fb1c52a82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_based_location(csv_data, file):\n",
    "    csv_data['group_ID'] = csv_data.groupby(['gpe_latitude', 'gpe_longitude']).ngroup()\n",
    "    csv_data.sort_values(by='group_ID', inplace=True)\n",
    "    csv_data.reset_index(drop=True, inplace=True)\n",
    "    save_file = \"grouped_data_\" + file + \".csv\"  \n",
    "    output_file_path = os.path.join(output_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"file saved after grouping \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "def find_most_relevant_gpe(group_df):\n",
    "    gpe_variations = group_df['gpe'].tolist()\n",
    "    best_match, _ = process.extractOne(group_df['gpe'].iloc[0], gpe_variations, scorer=fuzz.ratio)\n",
    "    group_df['gpe_new'] = best_match \n",
    "    return group_df  \n",
    "\n",
    "def format_gpe(csv_data, file):\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: re.sub(r\"'s$\", \"\", x))\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: x.title())\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: x.replace('@', ''))\n",
    "    csv_data = csv_data.loc[(csv_data['gpe_latitude'] != 0) & (csv_data['gpe_longitude'] != 0)]\n",
    "    print(\"done with setting gpename\")\n",
    "    csv_data = csv_data.groupby('group_ID').apply(find_most_relevant_gpe).reset_index(level=0, drop=True)\n",
    "    print(\"done with formatting\")\n",
    "    save_file = \"formatted_data_\" + file + \".csv\"  \n",
    "    output_file_path = os.path.join(output_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"File saved after formatting: \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "def sum_and_no_duplicates(csv_data,file):\n",
    "    csv_data['gpe_sum'] = csv_data.groupby(['gpe_new', 'group_ID'])['gpe_occurrences'].transform('sum')\n",
    "    csv_data = csv_data.drop_duplicates(subset=['gpe_new'], keep='first')\n",
    "    save_file = \"no_duplicates_\" + file + \".csv\" \n",
    "    output_file_path = os.path.join(final_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"file saved after sum_and_no_duplicates \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "# def is_location_with_spacy(word):\n",
    "#     doc = nlp(word)\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ == \"GPE\":\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def validate_gpe_with_spacy(csv_data, file):\n",
    "#     # Create a new column 'Is_Valid' in the DataFrame to store validation results\n",
    "#     csv_data['Is_Valid'] = csv_data['gpe_new'].apply(lambda x: is_location_with_spacy(x))\n",
    "#     # Convert 'Is_Valid' to 1 for valid and 0 for invalid\n",
    "#     csv_data['Is_Valid'] = csv_data['Is_Valid'].astype(int)\n",
    "#     save_file = \"valid_gpe_\" + file + \".csv\"\n",
    "#     output_file_path = os.path.join(output_folder, save_file)\n",
    "#     csv_data.to_csv(output_file_path, index=False)\n",
    "#     print(\"File saved after validate_gpe: \", save_file)\n",
    "#     return csv_data\n",
    "\n",
    "def create_heatmap(csv_data, file):\n",
    "    max_occurrences_row = csv_data.loc[csv_data['gpe_sum'].idxmax()]\n",
    "    highest_occurrences_latitude = max_occurrences_row['gpe_latitude']\n",
    "    highest_occurrences_longitude = max_occurrences_row['gpe_longitude']\n",
    "    highest_occurrence = [highest_occurrences_latitude, highest_occurrences_longitude]\n",
    "    print(highest_occurrence)\n",
    "    highest_gpe = max_occurrences_row['gpe_new']\n",
    "    print(highest_gpe)\n",
    "    heatmap_map = folium.Map(location=highest_occurrence, zoom_start=10, tiles='OpenStreetMap', max_zoom=5)\n",
    "    heat_data = list(zip(csv_data['gpe_latitude'], csv_data['gpe_longitude'], csv_data['gpe_sum']))\n",
    "    HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, min_opacity=0.5, radius=15, blur=20).add_to(heatmap_map)\n",
    "    disable_interactivity_js = \"\"\"\n",
    "    function disableInteractivity() {\n",
    "        var map = document.getElementById('map');\n",
    "        map.style['pointer-events'] = 'none';\n",
    "    }\n",
    "    disableInteractivity();\n",
    "    \"\"\"\n",
    "    \n",
    "    folium.Element(disable_interactivity_js).add_to(heatmap_map)\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 20px; width: 180px; \n",
    "                background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; z-index: 1000;\">\n",
    "        <div style=\"text-align: center; padding: 10px; font-size: 14px; font-weight: bold;\">Relative frequency of location count legend (0 to 1 Scale)</div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: blue; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0 - 0.4: Blue</span>\n",
    "        </div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: green; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0.4 - 0.65: Green</span>\n",
    "        </div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: red; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0.65 - 1: Red</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    save_file = \"heatmap_\" + file\n",
    "    heatmap_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "    #csv_data.to_csv(output_file_path, index=False)\n",
    "    heatmap_map.save(output_folder + save_file + '.html')\n",
    "    print(\"Heatmap will be saved to:\", output_folder + save_file + '.html')\n",
    "\n",
    "    return heatmap_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a079a39-c8cb-4bee-b39d-5f1d042f6599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved after grouping  grouped_data_tcnjsignal.csv\n",
      "done with setting gpename\n",
      "done with formatting\n",
      "File saved after formatting:  formatted_data_tcnjsignal.csv\n",
      "file saved after sum_and_no_duplicates  no_duplicates_tcnjsignal.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path where your CSV files are located\n",
    "# folder_path = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "\n",
    "csv_data = pd.read_csv('/Users/gayathri/Documents/GPE_intra_domain_cleaned/news_articles_info_www.tcnjsignal.net.csv')\n",
    "file = 'tcnjsignal'\n",
    "invalid_places = \"invalid_places\"+file\n",
    "csv_data = grouping_based_location(csv_data,file)\n",
    "csv_data = format_gpe(csv_data,file)\n",
    "csv_data = sum_and_no_duplicates(csv_data,file)\n",
    "#csv_data_heaptmap = create_heatmap(csv_data,file)\n",
    "#csv_data = validate_gpe_with_spacy(csv_data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b49c15b-7e04-4118-a324-af8950371e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CSV files in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned: 118\n",
      "Total number of CSV files in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data: 129\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_csv_files(folder_path):\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "    return len(csv_files)\n",
    "\n",
    "# Replace 'your_folder_path' with the actual path to your folder\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "results_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "\n",
    "try:\n",
    "    total_csv_files = count_csv_files(input_folder)\n",
    "    print(f'Total number of CSV files in {input_folder}: {total_csv_files}')\n",
    "    total_csv_files = count_csv_files(results_folder)\n",
    "    print(f'Total number of CSV files in {results_folder}: {total_csv_files}')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {folder_path} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd0913a-d451-4752-a175-69e565bdf73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names without \"news_articles_info_\" in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned:\n",
      "newjersey.news12.com.csv\n",
      "www.jerseyshoreonline.com.csv\n",
      "920thejersey.com.csv\n",
      "www.hobokenhorse.com.csv\n",
      "www.mycentraljersey.com.csv\n",
      "eccobserverdotcom.wordpress.com.csv\n",
      "wpgtalkradio.com.csv\n",
      "www.indiaabroad.com.csv\n",
      "www.wlvt.org.csv\n",
      "www.jerseyvoices.com.csv\n",
      "medium.com.csv\n",
      "www.stocktonargo.com.csv\n",
      "princetoninfo.com.csv\n",
      "www.951wayv.com.csv\n",
      "www.rahwayrising.com.csv\n",
      "www.dailyprincetonian.com.csv\n",
      "wpst.com.csv\n",
      "radio.rutgers.edu.csv\n",
      "www.westmilfordmessenger.com.csv\n",
      "www.shorelocalnews.com.csv\n",
      "pocono967.com.csv\n",
      "www1.nyc.gov.csv\n",
      "rennamedia.com.csv\n",
      "southjerseyobserver.com.csv\n",
      "newstalk990.com.csv\n",
      "www.anointedonline.net.csv\n",
      "thepakistaninewspaper.com.csv\n",
      "www.irishcentral.com.csv\n",
      "www.dirt-mag.com.csv\n",
      "www.civicstory.org.csv\n",
      "phl17.com.csv\n",
      "montclairdispatch.com.csv\n",
      "www.divyabhaskar.co.in.csv\n",
      "www.wnyc.org.csv\n",
      "www.mcccvoice.org.csv\n",
      "njrevolutionradio.com.csv\n",
      "www.pressofatlanticcity.com.csv\n",
      "www.tristatevoice.com.csv\n",
      "literock969.com.csv\n",
      "www.urbanagendamagazine.com.csv\n",
      "943thepoint.com.csv\n",
      "nepszava.u.csv\n",
      "www.lavocedinewyork.com.csv\n",
      "www.southjerseybiz.net.csv\n",
      "www.the-gazette-newspaper.com.csv\n",
      "downbeachbuzz.com.csv\n",
      "www.theyeshivaworld.com.csv\n",
      "www.newsindiatimes.com.csv\n",
      "newjerseyglobe.com.csv\n",
      "www.spartaindependent.com.csv\n",
      "www.law.com.csv\n",
      "1057thehawk.com.csv\n",
      "sojo1049.com.csv\n",
      "www.my9nj.com.csv\n",
      "www.posteaglenewspaper.com.csv\n",
      "gujaratdarpan.com.csv\n",
      "mycommunitysource.com.csv\n",
      "wurdradio.com.csv\n",
      "njbmagazine.com.csv\n",
      "987thecoast.com.csv\n",
      "www.recordonline.com.csv\n",
      "newtownpress.com.csv\n",
      "www.ahherald.com.csv\n",
      "www.wliw.org.csv\n",
      "njjewishnews.timesofisrael.com.csv\n",
      "latribunanj.com.csv\n",
      "www.phillyvoice.com.csv\n",
      "6abc.com.csv\n",
      "thepositivecommunity.com.csv\n",
      "www.wqxr.org.csv\n",
      "countywatchers.wordpress.com.csv\n",
      "www.tygodnikplus.com.csv\n",
      "essexnewsdaily.com.csv\n",
      "unionnewsdaily.com.csv\n",
      "hellenicnews.com.csv\n",
      "www.princetonmagazine.com.csv\n",
      "www.hammontongazette.com.csv\n",
      "www.roi-nj.com.csv\n",
      "fduequinox.wordpress.com.csv\n",
      "wsus1023.iheart.com.csv\n",
      "www.ntd.t.csv\n",
      "www.tcnjsignal.net.csv\n",
      "www.rcan.org.csv\n",
      "savejersey.com.csv\n",
      "www.courierpostonline.com.csv\n",
      "www.1077thebronc.com.csv\n",
      "brick.shorebeat.com.csv\n",
      "cccvoice.wordpress.com.csv\n",
      "philadelphia.cbslocal.com.csv\n",
      "www.wsou.net.csv\n",
      "www.visionsnewspaper.com.csv\n",
      "www.telemundo47.com.csv\n",
      "www.koreadailyus.com.csv\n",
      "www.theridernews.com.csv\n",
      "www.thedrewacorn.com.csv\n",
      "pinebarrenstribune.com.csv\n",
      "www.wvlt.com.csv\n",
      "www.townshipjournal.com.csv\n",
      "www.wwfm.org.csv\n",
      "www.themontynews.org.csv\n",
      "thecoaster.net.csv\n",
      "www.northjersey.com.csv\n",
      "wbjb.org.csv\n",
      "www.njpen.com.csv\n",
      "dailyvoice.com.csv\n",
      "forward.com.csv\n",
      "newjerseybuzz.com.csv\n",
      "www.towntopics.com.csv\n",
      "www.thecore.fm.csv\n",
      "www.classicoldieswmid.com.csv\n",
      "www.tapinto.net.csv\n",
      "am970theanswer.com.csv\n",
      "brigantinenow.com.csv\n",
      "thenjsentinel.com.csv\n",
      "baristanet.com.csv\n",
      "www.ocsentinel.com.csv\n",
      "www.theobserver.com.csv\n",
      "wputimes.wordpress.com.csv\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_file_name(file_name, prefix):\n",
    "    if file_name.startswith(prefix):\n",
    "        file_name = file_name[len(prefix):]\n",
    "    return file_name\n",
    "\n",
    "def get_files_without_prefix(folder_path, prefix):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    input_files = [process_file_name(file, prefix) for file in files]\n",
    "    return input_files\n",
    "\n",
    "# Replace 'your_folder_path' and 'news_articles_info_' with the actual folder path and prefix\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "prefix_to_remove = 'news_articles_info_'\n",
    "\n",
    "try:\n",
    "    input_files = get_files_without_prefix(input_folder, prefix_to_remove)\n",
    "    print(f'File names without \"{prefix_to_remove}\" in {input_folder}:')\n",
    "    for file_name in input_files:\n",
    "        print(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {input_folder} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "print(len(input_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07230df9-84d3-4622-89ce-e2c601419a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names without \"no_duplicates_\" in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data:\n",
      "brick.shorebeat.csv\n",
      "montynews.csv\n",
      "njjewishnews.timesofisrael.csv\n",
      "wpgtalkradio.csv\n",
      "www.recordonline.csv\n",
      "www.951wayv.csv\n",
      "wsou.csv\n",
      "dailyvoice.csv\n",
      "njrevolutionradio.csv\n",
      "www.roi-nj.csv\n",
      "divyabhaskar.csv\n",
      "literock969.csv\n",
      "cccvoice.wordpress.csv\n",
      "www.posteaglenewspaper.csv\n",
      "www.njpen.csv\n",
      "am970theanswer.csv\n",
      "civicstory.csv\n",
      "phl17.csv\n",
      "philadelphia.cbslocal.csv\n",
      "www.telemundo47.csv\n",
      "www.newsindiatimes.csv\n",
      "www.koreadailyus.csv\n",
      "www.stocktonargo.csv\n",
      "wlvt.csv\n",
      "newjerseybuzz.csv\n",
      "newtownpress.csv\n",
      "www.jerseyvoices.csv\n",
      "www.hobokenhorse.csv\n",
      "hellenicnews.csv\n",
      "1057thehawk.csv\n",
      "thepositivecommunity.csv\n",
      "thecoaster.csv\n",
      "tcnjsignal.csv\n",
      "www.law.csv\n",
      "brigantinenow.csv\n",
      "anointedonline.csv\n",
      "pinebarrenstribune.csv\n",
      "www.hammontongazette.csv\n",
      "savejersey.csv\n",
      "rcan.csv\n",
      "www.my9nj.csv\n",
      "forward.csv\n",
      "pocono967.csv\n",
      "wbjb.csv\n",
      "theobserver.csv\n",
      "www.princetonmagazine.csv\n",
      "radio.csv\n",
      "www.theridernews.csv\n",
      "mycommunitysource.csv\n",
      "fduequinox.wordpress.csv\n",
      "medium.csv\n",
      "thepakistaninewspaper.csv\n",
      "wliw.csv\n",
      "wwfm.csv\n",
      "sojo1049.csv\n",
      "www.northjersey.csv\n",
      "6abc.csv\n",
      "www.mycentraljersey.csv\n",
      "eccobserverdotcom.wordpress.csv\n",
      "www.townshipjournal.csv\n",
      "www.the-gazette-newspaper.csv\n",
      "www.1077thebronc.csv\n",
      "www.rahwayrising.csv\n",
      "nepszava.csv\n",
      "www.lavocedinewyork.csv\n",
      "www.indiaabroad.csv\n",
      "www.towntopics.csv\n",
      "rennamedia.csv\n",
      "www.tygodnikplus.csv\n",
      "www.shorelocalnews.csv\n",
      "www.visionsnewspaper.csv\n",
      "www.tristatevoice.csv\n",
      "baristanet.csv\n",
      "www.theyeshivaworld.csv\n",
      "wputimes.wordpress.csv\n",
      "newjerseyglobe.csv\n",
      "www.classicoldieswmid.csv\n",
      "www.pressofatlanticcity.csv\n",
      "www.jerseyshoreonline.csv\n",
      "newstalk990.csv\n",
      "latribunanj.csv\n",
      "www.wvlt.csv\n",
      "njbmagazine.csv\n",
      "920thejersey.csv\n",
      "wurdradio.csv\n",
      "www.urbanagendamagazine.csv\n",
      "www.dirt-mag.csv\n",
      "www.phillyvoice.csv\n",
      "wnyc.csv\n",
      "www.thedrewacorn.csv\n",
      "thenjsentinel.csv\n",
      "www.ocsentinel.csv\n",
      "montclairdispatch.csv\n",
      "987thecoast.csv\n",
      "www.spartaindependent.csv\n",
      "mcccvoice.csv\n",
      "www.courierpostonline.csv\n",
      "thecore.csv\n",
      "downbeachbuzz.csv\n",
      "www.ahherald.csv\n",
      "nyc.csv\n",
      "ntd.csv\n",
      "www.irishcentral.csv\n",
      "wpst.csv\n",
      "943thepoint.csv\n",
      "southjerseyobserver.csv\n",
      "wqxr.csv\n",
      "unionnewsdaily.csv\n",
      "www.westmilfordmessenger.csv\n",
      "essexnewsdaily.csv\n",
      "wsus1023.iheart.csv\n",
      "princetoninfo.csv\n",
      "tapinto.csv\n",
      "gujaratdarpan.csv\n",
      "newjersey.news12.csv\n",
      "www.dailyprincetonian.csv\n",
      "www.theobserver.csv\n",
      "southjerseybiz.csv\n",
      "countywatchers.wordpress.csv\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_file_name(file_name, prefix):\n",
    "    if file_name.startswith(prefix):\n",
    "        file_name = file_name[len(prefix):]\n",
    "    return file_name\n",
    "\n",
    "def get_files_without_prefix(folder_path, prefix):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    processed_files = [process_file_name(file, prefix) for file in files]\n",
    "    return processed_files\n",
    "\n",
    "# Replace 'your_folder_path' and 'news_articles_info_' with the actual folder path and prefix\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "prefix_to_remove = 'no_duplicates_'\n",
    "\n",
    "try:\n",
    "    processed_files = get_files_without_prefix(input_folder, prefix_to_remove)\n",
    "    print(f'File names without \"{prefix_to_remove}\" in {input_folder}:')\n",
    "    for file_name in processed_files:\n",
    "        print(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {input_folder} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "print(len(processed_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2cd7887-f9e7-466a-a7a1-f09b443aafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newjersey.news12.com.csv', 'www.jerseyshoreonline.com.csv', '920thejersey.com.csv', 'www.hobokenhorse.com.csv', 'www.mycentraljersey.com.csv', 'eccobserverdotcom.wordpress.com.csv', 'wpgtalkradio.com.csv', 'www.indiaabroad.com.csv', 'www.wlvt.org.csv', 'www.jerseyvoices.com.csv', 'medium.com.csv', 'www.stocktonargo.com.csv', 'princetoninfo.com.csv', 'www.951wayv.com.csv', 'www.rahwayrising.com.csv', 'www.dailyprincetonian.com.csv', 'wpst.com.csv', 'radio.rutgers.edu.csv', 'www.westmilfordmessenger.com.csv', 'www.shorelocalnews.com.csv', 'pocono967.com.csv', 'www1.nyc.gov.csv', 'rennamedia.com.csv', 'southjerseyobserver.com.csv', 'newstalk990.com.csv', 'www.anointedonline.net.csv', 'thepakistaninewspaper.com.csv', 'www.irishcentral.com.csv', 'www.dirt-mag.com.csv', 'www.civicstory.org.csv', 'phl17.com.csv', 'montclairdispatch.com.csv', 'www.divyabhaskar.co.in.csv', 'www.wnyc.org.csv', 'www.mcccvoice.org.csv', 'njrevolutionradio.com.csv', 'www.pressofatlanticcity.com.csv', 'www.tristatevoice.com.csv', 'literock969.com.csv', 'www.urbanagendamagazine.com.csv', '943thepoint.com.csv', 'nepszava.u.csv', 'www.lavocedinewyork.com.csv', 'www.southjerseybiz.net.csv', 'www.the-gazette-newspaper.com.csv', 'downbeachbuzz.com.csv', 'www.theyeshivaworld.com.csv', 'www.newsindiatimes.com.csv', 'newjerseyglobe.com.csv', 'www.spartaindependent.com.csv', 'www.law.com.csv', '1057thehawk.com.csv', 'sojo1049.com.csv', 'www.my9nj.com.csv', 'www.posteaglenewspaper.com.csv', 'gujaratdarpan.com.csv', 'mycommunitysource.com.csv', 'wurdradio.com.csv', 'njbmagazine.com.csv', '987thecoast.com.csv', 'www.recordonline.com.csv', 'newtownpress.com.csv', 'www.ahherald.com.csv', 'www.wliw.org.csv', 'njjewishnews.timesofisrael.com.csv', 'latribunanj.com.csv', 'www.phillyvoice.com.csv', '6abc.com.csv', 'thepositivecommunity.com.csv', 'www.wqxr.org.csv', 'countywatchers.wordpress.com.csv', 'www.tygodnikplus.com.csv', 'essexnewsdaily.com.csv', 'unionnewsdaily.com.csv', 'hellenicnews.com.csv', 'www.princetonmagazine.com.csv', 'www.hammontongazette.com.csv', 'www.roi-nj.com.csv', 'fduequinox.wordpress.com.csv', 'wsus1023.iheart.com.csv', 'www.ntd.t.csv', 'www.tcnjsignal.net.csv', 'www.rcan.org.csv', 'savejersey.com.csv', 'www.courierpostonline.com.csv', 'www.1077thebronc.com.csv', 'brick.shorebeat.com.csv', 'cccvoice.wordpress.com.csv', 'philadelphia.cbslocal.com.csv', 'www.wsou.net.csv', 'www.visionsnewspaper.com.csv', 'www.telemundo47.com.csv', 'www.koreadailyus.com.csv', 'www.theridernews.com.csv', 'www.thedrewacorn.com.csv', 'pinebarrenstribune.com.csv', 'www.wvlt.com.csv', 'www.townshipjournal.com.csv', 'www.wwfm.org.csv', 'www.themontynews.org.csv', 'thecoaster.net.csv', 'www.northjersey.com.csv', 'wbjb.org.csv', 'www.njpen.com.csv', 'dailyvoice.com.csv', 'forward.com.csv', 'newjerseybuzz.com.csv', 'www.towntopics.com.csv', 'www.thecore.fm.csv', 'www.classicoldieswmid.com.csv', 'www.tapinto.net.csv', 'am970theanswer.com.csv', 'brigantinenow.com.csv', 'thenjsentinel.com.csv', 'baristanet.com.csv', 'www.ocsentinel.com.csv', 'www.theobserver.com.csv', 'wputimes.wordpress.com.csv']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "result = [name for name in input_files if name not in processed_files]\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0280250c-e073-4557-8e6d-0a9a6f012705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result list to a CSV file\n",
    "output_file_path = 'processed_2.csv'\n",
    "\n",
    "with open(output_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Names in processed_files'])\n",
    "    for name in processed_files:\n",
    "        csv_writer.writerow([name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a82a898-c01c-4453-8559-f646007b5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result list to a CSV file\n",
    "output_file_path = 'input.csv'\n",
    "\n",
    "with open(output_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Names in input_files'])\n",
    "    for name in input_files:\n",
    "        csv_writer.writerow([name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e72bcbb3-0362-4bcb-854b-581a97654624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to /Users/gayathri/Documents/input.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_com(cell_value):\n",
    "    \"\"\"\n",
    "    Remove '.com' from the cell value.\n",
    "    \"\"\"\n",
    "    return cell_value.replace('.com', '')\n",
    "\n",
    "def process_csv_inplace(file_path):\n",
    "    \"\"\"\n",
    "    Process a CSV file in-place, removing '.com' from the 'Names' column.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Assuming the column you want to process is named 'Names'\n",
    "    df['Names in input_files'] = df['Names in input_files'].apply(remove_com)\n",
    "\n",
    "    # Write the updated DataFrame back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# Replace 'your_file.csv' with your actual file path\n",
    "csv_file_path = '/Users/gayathri/Documents/input.csv'\n",
    "\n",
    "process_csv_inplace(csv_file_path)\n",
    "print(f'Processed data saved to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b49e77ee-21d4-470b-9603-2e488869b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in /Users/gayathri/Documents/input.csv that are not in /Users/gayathri/Documents/processed.csv:\n",
      "www.rcan.org.csv\n",
      "www.themontynews.org.csv\n",
      "www1.nyc.gov.csv\n",
      "wbjb.org.csv\n",
      "www.divyabhaskar.co.in.csv\n",
      "www.civicstory.org.csv\n",
      "www.thecore.fm.csv\n",
      "nepszava.u.csv\n",
      "www.wqxr.org.csv\n",
      "Names in input_files\n",
      "www.southjerseybiz.net.csv\n",
      "www.wwfm.org.csv\n",
      "radio.rutgers.edu.csv\n",
      "www.mcccvoice.org.csv\n",
      "www.tapinto.net.csv\n",
      "www.ntd.t.csv\n",
      "www.wlvt.org.csv\n",
      "www.wnyc.org.csv\n",
      "www.wsou.net.csv\n",
      "www.wliw.org.csv\n",
      "www.anointedonline.net.csv\n",
      "thecoaster.net.csv\n",
      "www.tcnjsignal.net.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_missing_values(file1, file2, column_name):\n",
    "    \"\"\"\n",
    "    Find the missing values in column_name that are present in file1 and not in file2.\n",
    "    \"\"\"\n",
    "    # Read the CSV files into DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Get the values in column_name that are in file1 but not in file2\n",
    "    missing_values = set(df1[column_name]) - set(df2[column_name])\n",
    "\n",
    "    return list(missing_values)\n",
    "\n",
    "# Replace 'file1.csv', 'file2.csv', and 'column_name' with your actual file paths and column name\n",
    "file1_path = '/Users/gayathri/Documents/input.csv'\n",
    "file2_path = '/Users/gayathri/Documents/processed.csv'\n",
    "column_name = 'Names'\n",
    "\n",
    "missing_values = find_missing_values(file1_path, file2_path, column_name)\n",
    "\n",
    "print(f'Missing values in {file1_path} that are not in {file2_path}:')\n",
    "for value in missing_values:\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e51c7-5adf-4352-8904-98d524daa7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac9a8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from shapely import wkt\n",
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import random\n",
    "from spacy.lang.en.examples import sentences \n",
    "from geopy.distance import great_circle\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9e5b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Location', 'Latitude', 'Longitude', 'Count'], dtype='object')\n",
      "(1123, 4)\n"
     ]
    }
   ],
   "source": [
    "#Small analysis and processing task \n",
    "njglobe_1 = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/newjerseyglobe_locations_1.csv')\n",
    "print(njglobe_1.columns)\n",
    "print(njglobe_1.shape)\n",
    "#print(len(fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2ed2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 4)\n"
     ]
    }
   ],
   "source": [
    "njglobe_1 = njglobe_1.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "print(njglobe_1.shape)\n",
    "#So there are 23 rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227334b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 10\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Pick a specific color (e.g., red)\n",
    "color = \"#FF0000\"\n",
    "\n",
    "# Add circle markers for each location (dots) with the chosen color and labels\n",
    "for index, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    popup_text = f\"Location: {location}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=5, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"trial_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114e680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 10\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = njglobe_1[\"Count\"].max()\n",
    "\n",
    "# Function to map count values to marker sizes (adjust the scale factor as needed)\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 10  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Add circle markers for each location with varying size based on the count and labels\n",
    "for index, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"Count\"]\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    marker_size = get_marker_size(count)\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=\"#FF0000\", fill=True, fill_color=\"#FF0000\").add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"trial_3.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46051463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#FF0000\"  # Red color\n",
    "\n",
    "# Define a function to map count values to marker sizes\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 40  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = njglobe_1[\"Count\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"Count\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_3.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59e2a335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235439, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_abc = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/news_articles_info_6abc.com.csv')\n",
    "six_abc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac09bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235439, 9)\n"
     ]
    }
   ],
   "source": [
    "six_abc = six_abc.dropna(subset=[\"gpe_latitude\", \"gpe_longitude\"])\n",
    "print(six_abc.shape)\n",
    "#no null data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28978602",
   "metadata": {},
   "source": [
    "#Grouping the location names based on the same latitude and longitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d8cec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'grouped_data_six_abc.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on 'gpe_latitude' and 'gpe_longitude' and create a new column 'group_ID'\n",
    "six_abc['group_ID'] = six_abc.groupby(['gpe_latitude', 'gpe_longitude']).ngroup()\n",
    "\n",
    "# Sort the DataFrame based on the 'group_ID' column\n",
    "six_abc.sort_values(by='group_ID', inplace=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "six_abc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "six_abc.to_csv('grouped_data_six_abc.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'grouped_data_six_abc.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d66a5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['news_url', 'outlet', 'outlet_latitude', 'outlet_longitude', 'gpe',\n",
      "       'gpe_latitude', 'gpe_longitude', 'gpe_county', 'gpe_occurrences',\n",
      "       'group_ID'],\n",
      "      dtype='object')\n",
      "(235439, 10)\n"
     ]
    }
   ],
   "source": [
    "#Small analysis and processing task \n",
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/grouped_data_six_abc.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "#print(len(fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3859923",
   "metadata": {},
   "source": [
    "#Rename the gpe based on the common string for the same group_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3058a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('group_ID')['gpe'].apply(list).reset_index()\n",
    "\n",
    "# Function to find the common pattern among a list of strings\n",
    "def find_common_pattern(strings):\n",
    "    if len(strings) == 0:\n",
    "        return None\n",
    "\n",
    "    # Initialize SequenceMatcher with the first string\n",
    "    seq_matcher = SequenceMatcher(None, strings[0], \"\")\n",
    "\n",
    "    # Iterate through the rest of the strings and find the longest common substring\n",
    "    common_substring = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        seq_matcher.set_seq2(s)\n",
    "        match = seq_matcher.find_longest_match(0, len(strings[0]), 0, len(s))\n",
    "        if match.size > 0:\n",
    "            common_substring = strings[0][match.a: match.a + match.size]\n",
    "            break\n",
    "\n",
    "    # Check if the pattern is valid and appears in all strings\n",
    "    if common_substring and all(common_substring in s for s in strings):\n",
    "        return common_substring\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Find the common pattern for each group and rename the location names accordingly\n",
    "for _, group in grouped_df.iterrows():\n",
    "    group_id = group['group_ID']\n",
    "    locations = group['gpe']\n",
    "\n",
    "    common_pattern = find_common_pattern(locations)\n",
    "\n",
    "    if common_pattern:\n",
    "        # Update the location names with the common pattern\n",
    "        df.loc[df['group_ID'] == group_id, 'gpe'] = common_pattern\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('modified_location_data_six_abc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68c09bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235439, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085437de",
   "metadata": {},
   "source": [
    "#Delete duplicate values for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fb13fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/modified_location_data_six_abc.csv')\n",
    "# df.drop_duplicates(subset='gpe', keep='first', inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "586ef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'gpe' and calculate the sum of 'gpe_occurrences'\n",
    "grouped_df = df.groupby('gpe', as_index=False)['gpe_occurrences'].sum()\n",
    "\n",
    "# Merge the original DataFrame with the grouped DataFrame on 'gpe'\n",
    "merged_df = df.merge(grouped_df, on='gpe', how='left', suffixes=('', '_sum'))\n",
    "\n",
    "# Update 'gpe_occurrences' column with the calculated sum for the first occurrence\n",
    "merged_df.loc[merged_df.duplicated('gpe'), 'gpe_occurrences'] = merged_df['gpe_occurrences_sum']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(['gpe_occurrences_sum'], axis=1, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_data_sixabc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4e669f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'merged_data_sixabc_1.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Group by 'gpe' and calculate the sum of 'gpe_occurrences'\n",
    "grouped_df = df.groupby('gpe', as_index=False)['gpe_occurrences'].sum()\n",
    "\n",
    "# Merge the original DataFrame with the grouped DataFrame on 'gpe'\n",
    "merged_df = df.merge(grouped_df, on='gpe', how='left', suffixes=('', '_sum'))\n",
    "\n",
    "# Update 'gpe_occurrences' column with the calculated sum\n",
    "merged_df['gpe_occurrences'] = merged_df['gpe_occurrences_sum']\n",
    "\n",
    "# Drop the unnecessary column 'gpe_occurrences_sum'\n",
    "merged_df.drop(['gpe_occurrences_sum'], axis=1, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_data_sixabc_1.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'merged_data_sixabc_1.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d02cdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235439, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04a0b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2108, 10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_duplicates_df = merged_df.drop_duplicates(subset='gpe', keep='first')\n",
    "remove_duplicates_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a984663",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicates_df.to_csv('no_duplicates_found_sixabc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba219d",
   "metadata": {},
   "source": [
    "#Verify that all the GPE's are verified location, if not drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48b11242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/no_duplicates_found_sixabc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2cf8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fea76634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_location_with_geonames(word):\n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    is_proper_noun = any(pos in ['NNP', 'NNPS'] for _, pos in tagged_words)\n",
    "    \n",
    "    if not is_proper_noun:\n",
    "        return False\n",
    "\n",
    "    # Check if the word is a location using Geonames\n",
    "    geolocator = Nominatim(user_agent=\"location_checker\",timeout=8)\n",
    "    try:\n",
    "        location = geolocator.geocode(word)\n",
    "        if location is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except GeocoderTimedOut:\n",
    "        return is_location_with_geonames(word)  # Retry the request if a timeout occurs\n",
    "    \n",
    "# Create a boolean mask to indicate whether each row is a valid location or not\n",
    "valid_locations_mask = df['gpe'].apply(is_location_with_geonames)\n",
    "\n",
    "# Keep only the rows with valid locations and drop the rest\n",
    "df = df[valid_locations_mask]\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('valid_gpes_sixabc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23ca019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 10)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18aa625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixabc_valid = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/valid_gpes_sixabc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77c62cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixabc_valid_1 = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/valid_gpes_sixabc_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ee0f2",
   "metadata": {},
   "source": [
    "#Visulaising these gpe points on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78a0b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "map_center = [sixabc_valid_1[\"gpe_latitude\"].mean(), sixabc_valid_1[\"gpe_longitude\"].mean()]\n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#2F539B\" \n",
    "\n",
    "# Define a function to map count values to marker sizes\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 40  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = sixabc_valid_1[\"gpe_occurrences\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in sixabc_valid_1.iterrows():\n",
    "    location = row[\"gpe\"]\n",
    "    latitude = row[\"gpe_latitude\"]\n",
    "    longitude = row[\"gpe_longitude\"]\n",
    "    count = row[\"gpe_occurrences\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_sixabc_1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66687897",
   "metadata": {},
   "source": [
    "#Generating a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb27f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the data points\n",
    "center_latitude = sixabc_valid_1['gpe_latitude'].mean()\n",
    "center_longitude = sixabc_valid_1['gpe_longitude'].mean()\n",
    "heatmap_map = folium.Map(location=[center_latitude, center_longitude], zoom_start=2)\n",
    "\n",
    "# Create a list of tuples containing the Latitude and Longitude for each data point\n",
    "heat_data = list(zip(sixabc_valid_1['gpe_latitude'], sixabc_valid_1['gpe_longitude'], sixabc_valid_1['gpe_occurrences']))\n",
    "\n",
    "# Add the HeatMap layer to the map\n",
    "HeatMap(heat_data).add_to(heatmap_map)\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_sixabc_1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "365ee77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the data points\n",
    "center_latitude = sixabc_valid_1['gpe_latitude'].mean()\n",
    "center_longitude = sixabc_valid_1['gpe_longitude'].mean()\n",
    "heatmap_map = folium.Map(location=[center_latitude, center_longitude], zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(sixabc_valid_1['gpe_latitude'], sixabc_valid_1['gpe_longitude'], sixabc_valid_1['gpe_occurrences']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_sixabc_tuned_1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eda645",
   "metadata": {},
   "source": [
    "#Applying the logarithmic transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3763716",
   "metadata": {},
   "outputs": [],
   "source": [
    "sixabc_valid = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/valid_gpes_sixabc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d564ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                news_url    outlet  \\\n",
       "0     https://6abc.com/entertainment/metallica-delay...  6abc.com   \n",
       "1     https://6abc.com/sports/new-mls-playoff-format...  6abc.com   \n",
       "2     https://6abc.com/society/meghan-and-harry-phot...  6abc.com   \n",
       "3                             https://6abc.com/5840218/  6abc.com   \n",
       "4                         https://6abc.com/tag/inmates/  6abc.com   \n",
       "...                                                 ...       ...   \n",
       "1185                          https://6abc.com/5839122/  6abc.com   \n",
       "1186                          https://6abc.com/5643306/  6abc.com   \n",
       "1187  https://6abc.com/business/chick-fil-a-to-close...  6abc.com   \n",
       "1188  https://6abc.com/brexit-hangs-in-the-balance-a...  6abc.com   \n",
       "1189  https://6abc.com/food/6-great-spots-for-ice-cr...  6abc.com   \n",
       "\n",
       "      outlet_latitude  outlet_longitude                   gpe  gpe_latitude  \\\n",
       "0           41.128848        -74.686881           New Zealand    -40.900557   \n",
       "1           41.128848        -74.686881             Argentina    -38.416097   \n",
       "2           41.128848        -74.686881          South Africa    -30.559482   \n",
       "3           41.128848        -74.686881             Australia    -25.274398   \n",
       "4           41.128848        -74.686881                Brazil    -14.235004   \n",
       "...               ...               ...                   ...           ...   \n",
       "1185        41.128848        -74.686881  The United Kingdom's     55.378051   \n",
       "1186        41.128848        -74.686881    the United Kingdom     55.378051   \n",
       "1187        41.128848        -74.686881        United Kingdom     55.378051   \n",
       "1188        41.128848        -74.686881    The United Kingdom     55.378051   \n",
       "1189        41.128848        -74.686881                Canada     56.130366   \n",
       "\n",
       "      gpe_longitude gpe_county  gpe_occurrences  group_ID  log_gpe_occurrences  \n",
       "0        174.885971        NaN               51         0             3.931826  \n",
       "1        -63.616672        NaN                1         1             0.000000  \n",
       "2         22.937506        NaN               10         4             2.302585  \n",
       "3        133.775136        NaN              179         5             5.187386  \n",
       "4        -51.925280        NaN               14         7             2.639057  \n",
       "...             ...        ...              ...       ...                  ...  \n",
       "1185      -3.435973        NaN                3      1517             1.098612  \n",
       "1186      -3.435973        NaN                9      1517             2.197225  \n",
       "1187      -3.435973        NaN               13      1517             2.564949  \n",
       "1188      -3.435973        NaN                3      1517             1.098612  \n",
       "1189    -106.346771        NaN              122      1518             4.804021  \n",
       "\n",
       "[1190 rows x 11 columns]>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply log transformation to the 'gpe_occurrences' column and create a new column\n",
    "sixabc_valid['log_gpe_occurrences'] = np.log(sixabc_valid['gpe_occurrences'])\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "sixabc_valid.to_csv('map_view_sixabc.csv', index=False)\n",
    "sixabc_valid.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd9397e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "map_center = [sixabc_valid[\"gpe_latitude\"].mean(), sixabc_valid[\"gpe_longitude\"].mean()]\n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#2F539B\" \n",
    "\n",
    "# Define a function to map count values to marker sizes\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 40  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = sixabc_valid[\"log_gpe_occurrences\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in sixabc_valid.iterrows():\n",
    "    location = row[\"gpe\"]\n",
    "    latitude = row[\"gpe_latitude\"]\n",
    "    longitude = row[\"gpe_longitude\"]\n",
    "    count = row[\"log_gpe_occurrences\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_sixabc_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f320ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the mean latitude and longitude of the data points\n",
    "center_latitude = sixabc_valid['gpe_latitude'].mean()\n",
    "center_longitude = sixabc_valid['gpe_longitude'].mean()\n",
    "heatmap_map = folium.Map(location=[center_latitude, center_longitude], zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(sixabc_valid['gpe_latitude'], sixabc_valid['gpe_longitude'], sixabc_valid['log_gpe_occurrences']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_sixabc_tuned_2.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

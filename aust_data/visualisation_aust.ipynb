{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb624efe-d786-434a-84d9-2176dac71a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/gayathri/myenv/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: geopandas in /Users/gayathri/myenv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: folium in /Users/gayathri/myenv/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: shapely in /Users/gayathri/myenv/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: plotly in /Users/gayathri/myenv/lib/python3.9/site-packages (5.16.1)\n",
      "Requirement already satisfied: seaborn in /Users/gayathri/myenv/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in /Users/gayathri/myenv/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in /Users/gayathri/myenv/lib/python3.9/site-packages (1.25.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: fiona>=1.8.19 in /Users/gayathri/myenv/lib/python3.9/site-packages (from geopandas) (1.9.4.post1)\n",
      "Requirement already satisfied: pyproj>=3.0.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from geopandas) (3.6.0)\n",
      "Requirement already satisfied: packaging in /Users/gayathri/myenv/lib/python3.9/site-packages (from geopandas) (23.1)\n",
      "Requirement already satisfied: requests in /Users/gayathri/myenv/lib/python3.9/site-packages (from folium) (2.31.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from folium) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Users/gayathri/myenv/lib/python3.9/site-packages (from folium) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (6.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (0.7.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (6.8.0)\n",
      "Requirement already satisfied: certifi in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
      "Requirement already satisfied: click~=8.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from fiona>=1.8.19->geopandas) (1.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests->folium) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests->folium) (3.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/gayathri/myenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gayathri/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gayathri/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: jinja2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (58.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/gayathri/myenv/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/gayathri/myenv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gayathri/myenv/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/gayathri/myenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GeocoderTimedOut' from 'geopy.distance' (/Users/gayathri/myenv/lib/python3.9/site-packages/geopy/distance.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdifflib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequenceMatcher\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m great_circle, GeocoderTimedOut, GeocoderServiceError\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GeocoderTimedOut' from 'geopy.distance' (/Users/gayathri/myenv/lib/python3.9/site-packages/geopy/distance.py)"
     ]
    }
   ],
   "source": [
    "!pip install pandas geopandas folium shapely plotly seaborn matplotlib numpy\n",
    "\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from shapely import wkt\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "from spacy.lang.en.examples import sentences \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from difflib import SequenceMatcher\n",
    "from geopy.distance import great_circle, GeocoderTimedOut, GeocoderServiceError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e626f58c-b9a1-4c43-9ccb-7f7de5dc1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input CSV file paths\n",
    "input_files = [\n",
    "    '\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_1.csv',\n",
    "    '\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_2.csv',\n",
    "    '\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_3.csv',\n",
    "    '\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_4.csv',\n",
    "    '\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_5.csv',\n",
    "]\n",
    "\n",
    "# Output CSV file path\n",
    "output_file = 'aust_merged_data.csv'\n",
    "\n",
    "# Create an empty DataFrame to store the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Read and concatenate data from all input CSVs\n",
    "for input_file in input_files:\n",
    "    df = pd.read_csv(input_file)\n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "\n",
    "# Write the merged data to the output CSV file\n",
    "merged_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c8fc6b-bd55-465e-a851-dbdc3e7a2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GPE', 'Count', 'Latitude', 'Longitude'], dtype='object')\n",
      "(654, 4)\n"
     ]
    }
   ],
   "source": [
    "aust_data = pd.read_csv(\"\\\\Users\\\\gayat\\\\Desktop\\\\work\\\\aust_merged_data.csv\")\n",
    "print(aust_data.columns)\n",
    "print(aust_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653e9bc6-63c2-41f1-8599-97a490c23091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 4)\n",
      "(93, 4)\n",
      "(82, 4)\n",
      "(85, 4)\n",
      "(232, 4)\n"
     ]
    }
   ],
   "source": [
    "aust_data_1 = pd.read_csv('\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_1.csv')\n",
    "print(aust_data_1.shape)\n",
    "aust_data_2 = pd.read_csv('\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_2.csv')\n",
    "print(aust_data_2.shape)\n",
    "aust_data_3 = pd.read_csv('\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_3.csv')\n",
    "print(aust_data_3.shape)\n",
    "aust_data_4 = pd.read_csv('\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_4.csv')\n",
    "print(aust_data_4.shape)\n",
    "aust_data_5 = pd.read_csv('\\\\Users\\\\gayat\\\\Documents\\\\aust_data\\\\std_5.csv')\n",
    "print(aust_data_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785af1d2-f646-47e6-a9b5-36d247a94b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 4)\n"
     ]
    }
   ],
   "source": [
    "aust_data = aust_data.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "print(aust_data.shape)\n",
    "#There is no null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6563bcc-040e-4206-928f-3bdd46283e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80b2b065-ecfb-4834-97e5-cf338926dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'grouped_data_aust.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on 'gpe_latitude' and 'gpe_longitude' and create a new column 'group_ID'\n",
    "aust_data['group_ID'] = aust_data.groupby(['Latitude', 'Longitude']).ngroup()\n",
    "\n",
    "# Sort the DataFrame based on the 'group_ID' column\n",
    "aust_data.sort_values(by='group_ID', inplace=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "aust_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "aust_data.to_csv('grouped_data_aust.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'grouped_data_aust.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a510202-dffe-49d7-9e3b-7b7d509aedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aust_data = pd.read_csv('\\\\Users\\\\gayat\\\\Desktop\\\\work\\\\grouped_data_aust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e83b75c1-4f19-45dd-b61d-c2a25c4a9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove 's(apostrophe s) from the GPE name => Formatting the data step 1.\n",
    "aust_data['GPE'] = aust_data['GPE'].apply(lambda x: re.sub(r\"'s$\", \"\", x))\n",
    "aust_data.to_csv('format_aust_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c570177-4c2c-4a40-9fa1-e36422c8aa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    South Warrnambool\n",
      "Name: GPE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = {'GPE': [\"South Warrnambool's\"]}\n",
    "df = pd.DataFrame(data)\n",
    "df['GPE'] = df['GPE'].apply(lambda x: re.sub(r\"'s$\", \"\", x))\n",
    "print(df['GPE'])\n",
    "#Sample of how the above code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1340f65d-f09e-4ee8-b64d-1fa2831135a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to convert GPE name to proper noun => new york city will be converted to New York City. this helps to get the places recongnised by the geopy database.\n",
    "aust_data['GPE'] = aust_data['GPE'].apply(lambda x: x.title())\n",
    "aust_data.to_csv('format_aust_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09581612-72dc-4ab1-b9d8-c038611099ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('\\\\Users\\\\gayat\\\\Desktop\\\\work\\\\format_aust_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfdaef67-f585-4f65-9180-52fe2a9e0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'GPE' and 'group_ID', calculate the sum of 'Count' for each group\n",
    "grouped = df.groupby(['GPE', 'group_ID'])['Count'].sum().reset_index()\n",
    "\n",
    "# Merge the grouped DataFrame back to the original DataFrame based on 'GPE' and 'group_ID'\n",
    "df = df.merge(grouped, on=['GPE', 'group_ID'], suffixes=('', '_sum'))\n",
    "\n",
    "# Update the 'Count' column with the calculated sum\n",
    "df['Count'] = df['Count_sum']\n",
    "\n",
    "# Drop the temporary 'Count_sum' column\n",
    "df.drop(columns=['Count_sum'], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('format_aust_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9d732d-0014-4cb3-b43c-63bea8643fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/format_aust_3.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce40042-2195-4431-a1a4-08e71bfbe02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates = df.drop_duplicates(subset=['GPE'], keep='first')\n",
    "df_no_duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9caf120-2361-4ef9-8eb0-39829f31032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if everything is a valid location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6224dee0-f61b-4092-97f3-32d66dcecf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8cf2de-5b19-4e30-bc6d-de06afa404c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Valid Places: 157\n",
      "Count of Invalid Places: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/xf93n76s47j1cw7f6v661b040000gn/T/ipykernel_43533/2547060998.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['Is_Valid'] = df_no_duplicates['GPE'].apply(lambda x: is_location_with_geonames(x, invalid_places))\n",
      "/var/folders/rz/xf93n76s47j1cw7f6v661b040000gn/T/ipykernel_43533/2547060998.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['Category'] = df_no_duplicates['Is_Valid'].apply(lambda x: 1 if x else 0)\n"
     ]
    }
   ],
   "source": [
    "def is_location_with_geonames(word, invalid_places):\n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    is_proper_noun = any(pos in ['NNP', 'NNPS'] for _, pos in tagged_words)\n",
    "    \n",
    "    if not is_proper_noun:\n",
    "        return False\n",
    "\n",
    "    # Check if the word is a location using Geonames\n",
    "    geolocator = Nominatim(user_agent=\"location_checker\",timeout=8)\n",
    "    try:\n",
    "        location = geolocator.geocode(word)\n",
    "        if location is not None:\n",
    "            return True\n",
    "        else:\n",
    "            invalid_places.append(word)  # Add to invalid list\n",
    "            return False\n",
    "    except GeocoderTimedOut:\n",
    "        return is_location_with_geonames(word, invalid_places)  # Retry the request if a timeout occurs\n",
    "\n",
    "# Create empty lists for valid and invalid places\n",
    "valid_places = []\n",
    "invalid_places = []\n",
    "\n",
    "# Apply the function to create a boolean mask and populate invalid_places list\n",
    "df_no_duplicates['Is_Valid'] = df_no_duplicates['GPE'].apply(lambda x: is_location_with_geonames(x, invalid_places))\n",
    "\n",
    "# Create a 'Category' column where 1 represents valid locations and 0 represents invalid locations\n",
    "df_no_duplicates['Category'] = df_no_duplicates['Is_Valid'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Save the df_no_duplicates DataFrame with the 'Category' column\n",
    "df_no_duplicates.to_csv('df_no_duplicates_with_category.csv', index=False)\n",
    "\n",
    "# Save the modified dataframe with valid locations to a new CSV file\n",
    "df_valid = df_no_duplicates[df_no_duplicates['Is_Valid']]\n",
    "df_valid.to_csv('valid_gpe_aust.csv', index=False)\n",
    "\n",
    "# Save the list of invalid places to a separate CSV file\n",
    "invalid_places_df = pd.DataFrame({'Invalid_Places': invalid_places})\n",
    "invalid_places_df.to_csv('invalid_gpe_aust.csv', index=False)\n",
    "\n",
    "# Get the count of valid and invalid places\n",
    "count_valid_places = df_no_duplicates['Is_Valid'].sum()\n",
    "count_invalid_places = len(invalid_places)\n",
    "\n",
    "# Print the count of valid and invalid places\n",
    "print(\"Count of Valid Places:\", count_valid_places)\n",
    "print(\"Count of Invalid Places:\", count_invalid_places)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33d0a910-2e19-4c1f-bcd8-622a7d073529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique GPE Names in df_no_duplicates but not in valid_gpe_aust_1.csv and invalid_places.csv:\n",
      "Us\n",
      "Previ­\n",
      "Turkey\n",
      "53,9\n",
      "Navato\n",
      "Brisbane\n",
      "Vancouver\n",
      "Goulburn\n",
      "Mornington\n",
      "Albury\n",
      "Motza\n",
      "Mt!Scamnto\n",
      "Hellekens\n",
      "Regensburg\n",
      "Nice\n",
      "Banyule\n",
      "Island\n",
      "Tht Dominican\n",
      "Sonoma\n",
      "Sudan\n",
      "Naracoorte\n",
      "Pgav\n",
      "Merrivale\n",
      "Wtll!W\\\n",
      "Maribymong\n",
      "Philippines\n",
      "Charlotte\n",
      "Lurn\n",
      "Thei.R\n",
      "Agra\n",
      "Rothenburg\n",
      "Ri\n",
      "Bergen\n",
      "Lagos\n",
      "Puglia\n",
      "M.Monaghan\n",
      "Stockholm\n",
      "Delhi\n",
      "Solllliern Grampians\n",
      "Jaipur\n",
      "Butte\n",
      "Collingwood\n",
      "Dandenong\n",
      "Kavanas\n",
      "Belarus\n",
      "Lyon\n",
      "Manitoba\n",
      "Baynes\n",
      "Vegas\n",
      "Heywood-Portland\n",
      "Peterborough\n",
      "Jfranklin\n",
      "Perth\n",
      "Dreambound\n",
      "Winocka\n",
      "Queensland\n",
      "Coleraine\n",
      "Queens­\n",
      "Warmambool\n",
      "Tltis\n",
      "Austraiia\n",
      "Pomborneit\n",
      "Tum\n",
      "Australia.8.30\n",
      "Terrey\n",
      "Adelaide\n",
      "Catania\n",
      "Brooklyn\n",
      "Mghl\n",
      "Portland\n",
      "Rome\n",
      "Pemberton\n",
      "Differenl\n",
      "Goodc\n",
      "Cranboume\n",
      "Cooma\n",
      "Berryt\n",
      "Sc\n",
      "Coburg\n",
      "Sa.So\n",
      "Westmere\n",
      "Aghl\n",
      "Nc\n",
      "Yorkshire\n",
      "Shortland\n",
      "Shoalhaven\n",
      "Kansas\n",
      "Bushfield\n",
      "Parap\n",
      "Denali\n",
      "Jlor\n",
      "Mornlngton\n",
      "Euston\n",
      "Nhulunbuy\n",
      "Indio\n",
      "Ataili\n",
      "North­\n",
      "Ottawa\n",
      "Submer\n",
      "Glencoe\n",
      "Cairns\n",
      "Whitehaven\n",
      "Wooiey\n",
      "Mortlake\n",
      "Pakenham\n",
      "Bordeaux\n",
      "Bools\n",
      "Quebec\n",
      "Providence\n",
      "Syria\n",
      "Territo­\n",
      "Mildura\n",
      "Onelor\n",
      "Melbourne\n",
      "Bali\n",
      "Tyrendorra\n",
      "Sydney\n",
      "Branxholme\n",
      "Holland\n",
      "Marwick\n",
      "Guernsey\n",
      "Dhaka\n",
      "Flemington\n",
      "Logans Beach\n",
      "Lae\n",
      "Narrawong\n",
      "Solu­\n",
      "Acros\n",
      "Dunkeld\n",
      "Egypt\n",
      "S.Iicramenlo\n",
      "Camperdown\n",
      "Meua\n",
      "Heavyground\n",
      "Heywood\n",
      "Fremantle\n",
      "Wayalong\n",
      "Tassie\n",
      "Amecourt\n",
      "Victoria Point\n",
      "Edlhnlew1De\n",
      "Pakistan\n",
      "Ptav\n",
      "Bath\n",
      "Mali\n",
      "Wyong}\n",
      "Zaporizhzhia\n",
      "Huatulco\n",
      "Subiaco\n",
      "Llrving\n",
      "Ellerslie\n",
      "Dennington\n",
      "Wurzburg\n",
      "Bjue\n",
      "Ukraine\n",
      "Malibu\n",
      "Nj\n",
      "Fll\n",
      "Rosehill\n",
      "Northmelbourne\n",
      "Wanong\n",
      "Salem\n",
      "Miami\n",
      "Jenin\n",
      "Haintz\n",
      "Panmure\n",
      "Toorak\n",
      "Warrnambocl\n",
      "Indonesia\n",
      "Richmond\n",
      "Orleans\n",
      "Thmgs 1Hat\n",
      "Mpowerlnc\n",
      "Mool­\n",
      "Puerto\n",
      "Brookwood\n",
      "Nmccartney\n",
      "Seoul\n",
      "Tatura\n",
      "Gabba\n",
      "Canberra\n",
      "Pl!Dto\n",
      "Cork\n",
      "Hobart\n",
      "Edgbaston\n",
      "Gippsland\n",
      "Brambuk\n",
      "Florence\n",
      "Noosa\n",
      "Wooonoa\n",
      "Caramut\n",
      "Ne-Nc\n",
      "Ilbnotmilil\n",
      "Blnqerlnll\n",
      "Pineview\n",
      "Tenn\n",
      "Newfoundland\n",
      "Russells Creek\n",
      "Choclyn\n",
      "Delaware\n",
      "Princetown\n",
      "Blackheath\n",
      "Mumbai\n",
      "Mooloolaba\n",
      "Wonthaggi\n",
      "Queens\n",
      "Bitung\n",
      "Woombye\n",
      "Tasmania\n",
      "Redesdale\n",
      "N.Murphy\n",
      "Georgia\n",
      "Maryborough\n",
      "Terang\n",
      "Againsl\n",
      "Connecticut\n",
      "Cun\n",
      "Winnipeg\n",
      "Suva\n",
      "Puntarenas\n",
      "Davenport\n",
      "Ca\n",
      "Cartagena\n",
      "Panama\n",
      "Scotland\n",
      "Warrnambool\n",
      "Wfll\n",
      "Broadwater\n",
      "Sofitt\n",
      "Picton\n",
      "Lecce\n",
      "I.Sa\n",
      "Saint Martins\n",
      "Lieelong\n",
      "Mals+L\n",
      "T.Ste\n",
      "Nic:Es\n",
      "Maribyrnong\n",
      "Usa\n",
      "Bewong\n",
      "Nicaragua\n",
      "Ounkeld\n",
      "Norfolk\n",
      "Yarrawonga\n",
      "Sonora\n",
      "Albany\n",
      "Russia\n",
      "Nsw\n",
      "Norway\n",
      "Victoria\n",
      "Warwick\n",
      "Grassmere\n",
      "Sheffield\n"
     ]
    }
   ],
   "source": [
    "# Load 'valid_gpe_aust_1.csv' and 'invalid_places.csv' into DataFrames\n",
    "df_valid_gpe = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/valid_gpe_aust.csv')\n",
    "df_invalid_places = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/invalid_gpe_aust.csv')\n",
    "\n",
    "# Create sets of GPE names from the DataFrames\n",
    "valid_gpe_set = set(df_valid_gpe['GPE'])\n",
    "invalid_places_set = set(df_invalid_places['Invalid_Places'])\n",
    "\n",
    "# Create a set of GPE names from 'df_no_duplicates'\n",
    "df_no_duplicates_gpe_set = set(df_no_duplicates['GPE'])\n",
    "\n",
    "# Find GPE names present in 'df_no_duplicates' but not in both 'valid_gpe_aust_1.csv' and 'invalid_places.csv'\n",
    "unique_gpe_names = df_no_duplicates_gpe_set.difference(valid_gpe_set.union(invalid_places_set))\n",
    "\n",
    "# Convert the unique GPE names to a list\n",
    "unique_gpe_names_list = list(unique_gpe_names)\n",
    "\n",
    "# We noticed that the count of valid and invalid is not equal to the total GPE_names, hence printing the missed one's\n",
    "print(\"Unique GPE Names in df_no_duplicates but not in valid_gpe_aust_1.csv and invalid_places.csv:\")\n",
    "for name in unique_gpe_names_list:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62784870-16a7-49c9-bd34-82e79f4e562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of unique GPE names\n",
    "unique_gpe_df = pd.DataFrame({'Unique_GPE_Names': unique_gpe_names_list})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "unique_gpe_df.to_csv('missing_gpe_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cccce750-9790-4232-ac37-8cc7ab39349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/missing_gpe_names.csv')\n",
    "missing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f74dafc5-a22f-4a43-b196-a05570b6e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try with geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a32108-72d8-4249-9c1c-0ce1aa1c7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3\n",
    "api_key = GoogleV3(api_key=\"AIzaSyBJ8P42fvaYv5pmqNdEqYEOJPENnm7eND0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "058f84d2-9fdb-44a7-8ed0-c5e698e48bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/format_aust_3.csv')\n",
    "df.shape\n",
    "df = df.drop_duplicates(subset=['GPE'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7cb35d-7506-40a4-a3e1-394ce4429444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_location_with_geonames(word):\n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    is_proper_noun = any(pos in ['NNP', 'NNPS'] for _, pos in tagged_words)\n",
    "    \n",
    "    if not is_proper_noun:\n",
    "        return False\n",
    "\n",
    "    # Check if the word is a location using Geonames\n",
    "    geolocator = Nominatim(user_agent=\"location_checker\",timeout=8)\n",
    "    try:\n",
    "        location = geolocator.geocode(word)\n",
    "        if location is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except GeocoderTimedOut:\n",
    "        return is_location_with_geonames(word)  # Retry the request if a timeout occurs\n",
    "    \n",
    "# Create a boolean mask to indicate whether each row is a valid location or not\n",
    "valid_locations_mask = df_no_duplicates['GPE'].apply(is_location_with_geonames)\n",
    "\n",
    "# Keep only the rows with valid locations and drop the rest\n",
    "df_no_duplicates = df_no_duplicates[valid_locations_mask]\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df_no_duplicates.to_csv('valid_gpe_aust.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd7387f-2a91-4c0b-b554-f01277f39474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4c6a1-6318-4a29-910a-1af2979518a8",
   "metadata": {},
   "source": [
    "As per the filtered data ffrom valid_missing_gpe's let's try to find the latitude and longitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ee240c-4297-4ab7-909f-58e1624333be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3\n",
    "geolocator = GoogleV3(api_key=\"AIzaSyBJ8P42fvaYv5pmqNdEqYEOJPENnm7eND0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578b9baa-e704-49bb-8da8-1d22366fac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geolocation(loc_name):\n",
    "    '''\n",
    "    This function takes in a location name and returns the latitude and longitude of the location\n",
    "    '''\n",
    "    location = geolocator.geocode(loc_name)\n",
    "    return (location.latitude, location.longitude) if location else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af3ad6d-a80f-40de-8040-8fb5e03d023c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "valid_missing_gpe = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/valid_missing_gpe.csv')\n",
    "print(len(valid_missing_gpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23452653-ab23-4d74-b5d2-b1bb672480f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Turkey': (38.963745, 35.243322), 'Brisbane': (-27.4704528, 153.0260341), 'Vancouver': (49.2827291, -123.1207375), 'Mornington': (-38.22882, 145.0600448), 'Albury': (-36.0751193, 146.9094852), 'Motza': (31.794407, 35.168509), 'France': (46.227638, 2.213749), 'Sudan': (12.862807, 30.217636), 'Naracoorte': (-36.9569884, 140.7417779), 'Maribymong': (-37.7837482, 144.8908713), 'Philippines': (12.879721, 121.774017), 'Charlotte': (35.2270869, -80.8431267), 'Rothenburg': (49.3801834, 10.1867388), 'Bergen': (60.39126279999999, 5.3220544), 'Lagos': (6.5243793, 3.3792057), 'Puglia': (40.7928393, 17.1011931), 'Stockholm': (59.32932349999999, 18.0685808), 'Delhi': (28.7040592, 77.10249019999999), 'Jaipur': (26.9124336, 75.7872709), 'Collingwood': (44.50076869999999, -80.2169047), 'Dandenong': (-37.9847811, 145.2139907), 'Belarus': (53.709807, 27.953389), 'Lyon': (45.764043, 4.835659), 'Manitoba': (53.7608608, -98.81387629999999), 'Vegas': (36.171563, -115.1391009), 'Peterborough': (52.57031689999999, -0.2407996999999999), 'Perth': (-31.9523123, 115.861309), 'Queensland': (-22.575197, 144.0847926), 'Coleraine': (55.1325802, -6.6646102), 'Warmambool': (-38.3720954, 142.4777782), 'Austraiia': (-25.274398, 133.775136), 'Terrey Hills': (-33.6832036, 151.2254937), 'Adelaide': (-34.9284989, 138.6007456), 'Catania': (37.5078772, 15.0830304), 'Brooklyn': (40.6781784, -73.9441579), 'Portland': (45.515232, -122.6783853), 'Rome': (41.9027835, 12.4963655), 'Pemberton': (-34.4455602, 116.0332544), 'Cranbourne': (-38.1054204, 145.2818338), 'Cooma': (-36.2345567, 149.1270444), 'Westmere': (42.6911891, -73.8687343), 'Yorkshire': (53.9590858, -1.0792403), 'Shortland': (-32.8797702, 151.6937761), 'Shoalhaven': (-35.0810677, 150.4892292), 'Kansas': (39.011902, -98.4842465), 'Bushfield': (-38.3250935, 142.5055545), 'Mornlngton': (-38.22882, 145.0600448), 'Euston': (51.5280991, -0.1332084), 'Ottawa': (45.4215296, -75.69719309999999), 'Glencoe': (56.68255989999999, -5.1022713), 'Cairns': (-16.9203476, 145.7709529), 'Whitehaven': (54.549699, -3.589233), 'Mortlake': (51.4687363, -0.2627417), 'Pakenham': (-38.0776708, 145.4818724), 'Bordeaux': (44.837789, -0.57918), 'Quebec': (46.8130816, -71.20745959999999), 'Providence': (41.8239891, -71.4128343), 'Syria': (34.80207499999999, 38.996815), 'Melbourne': (-37.8136276, 144.9630576), 'Bali': (-8.4095178, 115.188916), 'Sydney': (-33.8688197, 151.2092955), 'Holland': (52.132633, 5.291265999999999), 'Guernsey': (49.4481982, -2.58949), 'Flemington': (40.5123258, -74.85933179999999), 'Logans Beach': (-38.4014129, 142.5178222), 'Egypt': (26.820553, 30.802498), 'Camperdown': (-33.88622429999999, 151.1791091), 'Heywood': (53.592628, -2.22565), 'Fremantle': (-32.0517901, 115.755096), 'Wayalong': (-33.9223568, 147.2026202), 'Victoria Point': (-27.5879419, 153.303966), 'Pakistan': (30.375321, 69.34511599999999), 'Mali': (17.570692, -3.996166), 'Dennington': (52.253926, 1.3393709), 'Ukraine': (48.379433, 31.16558), 'Malibu': (34.0380585, -118.6923438), 'Rosehill': (-33.8239653, 151.0212029), 'Indonesia': (-0.789275, 113.921327), 'Richmond': (37.5407246, -77.4360481), 'Seoul': (37.5518911, 126.9917937), 'Canberra': (-35.2809368, 149.1300092), 'Hobart': (-42.8826055, 147.3257196), 'Florence': (43.7695604, 11.2558136), 'Noosa': (-26.3645433, 152.9676695), 'Caramut': (-37.9590175, 142.5189691), 'Newfoundland': (53.49436391800336, -60.22052574294098), 'Russells Creek': (-36.34171507511021, 149.7011392578551), 'Delaware': (38.9108325, -75.52766989999999), 'Blackheath': (51.4658393, 0.0090338), 'Mumbai': (19.0759837, 72.8776559), 'Queens': (40.7282239, -73.7948516), 'Tasmania': (-42.0409059, 146.8087322), 'Connecticut': (41.6032207, -73.087749), 'Winnipeg': (49.8954221, -97.1385145), 'Davenport': (41.5236437, -90.5776367), 'Scotland': (56.49067119999999, -4.2026458), 'Warrnambool': (-38.3720954, 142.4777782), 'Picton': (-41.2954812, 174.0028153), 'Nicaragua': (12.865416, -85.207229), 'Yarrawonga': (-36.0260723, 146.0003241), 'Russia': (61.52401, 105.318756), 'Norway': (60.47202399999999, 8.468945999999999), 'Warwick': (52.28231599999999, -1.584927), 'Sheffield': (53.38112899999999, -1.470085)}\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dictionary to store the mapping between gpe_name and lat_long\n",
    "lat_long_mapping = {}\n",
    "\n",
    "# Iterate through the 'Unique_GPE_Names' and get the lat_long for each gpe_name\n",
    "for gpe_name in valid_missing_gpe['Unique_GPE_Names']:\n",
    "    lat_long = get_geolocation(gpe_name)\n",
    "    if lat_long:\n",
    "        lat_long_mapping[gpe_name] = lat_long\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(lat_long_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15e25b2-59d0-4755-8ddc-08d32de350fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to missing_valid_lat_long_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path where you want to save the CSV file\n",
    "csv_file_path = 'missing_valid_lat_long_mapping.csv'\n",
    "\n",
    "# Write the dictionary to the CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['gpe_name', 'latitude', 'longitude']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data rows\n",
    "    for gpe_name, (latitude, longitude) in lat_long_mapping.items():\n",
    "        writer.writerow({'gpe_name': gpe_name, 'latitude': latitude, 'longitude': longitude})\n",
    "\n",
    "print(f'Data has been written to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcaf11f-1d06-495d-955f-628dd3e6f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_gpe = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/valid_gpe_aust.csv')\n",
    "# Select only the desired columns\n",
    "selected_columns = ['GPE', 'Latitude', 'Longitude']\n",
    "df_valid_gpe = df_valid_gpe[selected_columns]\n",
    "# Save the DataFrame with the selected columns back to a CSV file\n",
    "df_valid_gpe.to_csv('valid_aust_gpe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c97f95-f3a5-41d3-aeef-a572e4d76d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "df_valid_missing_gpe = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/missing_valid_lat_long_mapping.csv')\n",
    "print(len(df_valid_missing_gpe))\n",
    "# Define a dictionary to map the old column names to the new ones\n",
    "column_name_mapping = {\n",
    "    'gpe_name': 'GPE',\n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "}\n",
    "# Rename the columns using the rename method\n",
    "df_valid_missing_gpe.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "# Save the DataFrame with the updated header names back to a CSV file\n",
    "df_valid_missing_gpe.to_csv('missing_valid_lat_long_mapping_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8f5d882-65e3-4c28-80c7-11f74bd4b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_missing_gpe =  pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/missing_valid_lat_long_mapping_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55a914e7-8ca9-42e0-92d5-8bfd3a6e0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([df_valid_gpe, df_valid_missing_gpe], ignore_index=True)\n",
    "concatenated_df.to_csv('aust_valid_gpe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ee75e13-bfba-475f-9b1b-d0029a0375db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(concatenated_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59489ee1-7dc8-41f5-89f5-3cb7cd9ced9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aust_gpe_data =  pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/aust_valid_gpe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5549226-fb0a-4ef6-b7cc-725b691be1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aust_gpe_data['GPE'] = aust_gpe_data['GPE'].apply(lambda x: re.sub(r\"'s$\", \"\", x))\n",
    "aust_gpe_data['GPE'] = aust_gpe_data['GPE'].apply(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bde3f54c-ada9-4e36-983d-32a182e5fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'grouped_data_aust_gpe_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on 'gpe_latitude' and 'gpe_longitude' and create a new column 'group_ID'\n",
    "aust_gpe_data['group_ID'] = aust_gpe_data.groupby(['Latitude', 'Longitude']).ngroup()\n",
    "\n",
    "# Sort the DataFrame based on the 'group_ID' column\n",
    "aust_gpe_data.sort_values(by='group_ID', inplace=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "aust_gpe_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "aust_gpe_data.to_csv('grouped_data_aust_gpe_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'grouped_data_aust_gpe_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a92fd4-1e1f-406e-b8f9-c72767b1e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GPE', 'Latitude', 'Longitude', 'group_ID'], dtype='object')\n",
      "(261, 4)\n"
     ]
    }
   ],
   "source": [
    "#Small analysis and processing task \n",
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/grouped_data_aust_gpe_data.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "#print(len(fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2442f4d0-3aef-4f30-b96d-89385d8b9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('group_ID')['GPE'].apply(list).reset_index()\n",
    "\n",
    "# Function to find the common pattern among a list of strings\n",
    "def find_common_pattern(strings):\n",
    "    if len(strings) == 0:\n",
    "        return None\n",
    "\n",
    "    # Initialize SequenceMatcher with the first string\n",
    "    seq_matcher = SequenceMatcher(None, strings[0], \"\")\n",
    "\n",
    "    # Iterate through the rest of the strings and find the longest common substring\n",
    "    common_substring = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        seq_matcher.set_seq2(s)\n",
    "        match = seq_matcher.find_longest_match(0, len(strings[0]), 0, len(s))\n",
    "        if match.size > 0:\n",
    "            common_substring = strings[0][match.a: match.a + match.size]\n",
    "            break\n",
    "\n",
    "    # Check if the pattern is valid and appears in all strings\n",
    "    if common_substring and all(common_substring in s for s in strings):\n",
    "        return common_substring\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Find the common pattern for each group and rename the location names accordingly\n",
    "for _, group in grouped_df.iterrows():\n",
    "    group_id = group['group_ID']\n",
    "    locations = group['GPE']\n",
    "\n",
    "    common_pattern = find_common_pattern(locations)\n",
    "\n",
    "    if common_pattern:\n",
    "        # Update the location names with the common pattern\n",
    "        df.loc[df['group_ID'] == group_id, 'GPE'] = common_pattern\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('grouped_data_aust_gpe_data_renamed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1229edc8-177a-449f-8dcf-f74110a29831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70e1cd-05fb-472c-aade-f57b60f546ea",
   "metadata": {},
   "source": [
    "#I have manually edited some of the GPE names which are going wrong after the above step. \n",
    "#Delete the duplicate rows by adding the frequency value to the first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a81cb0d9-ac49-48a5-9318-6079ae3e978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/grouped_data_aust_gpe_data_renamed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "103342d6-94de-4d2c-810a-90653330ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been updated with the \"sum\" column.\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'GPE' and 'group_ID' and count the rows in each group\n",
    "grouped = df.groupby(['GPE', 'group_ID']).size().reset_index(name='sum')\n",
    "# Merge the 'sum' column back into the original DataFrame\n",
    "df = df.merge(grouped, on=['GPE', 'group_ID'], how='left')\n",
    "\n",
    "# Save the DataFrame with the 'sum' column added to a new CSV file\n",
    "df.to_csv('aust_gpe_data_with_sum.csv', index=False)\n",
    "print('CSV file has been updated with the \"sum\" column.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3534f7c-10b3-4e38-a67b-ecbc329fa86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/aust_gpe_data_with_sum.csv')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7606bd21-23c6-4449-b44e-7fcefef01739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates have been removed, and the CSV file has been saved without duplicates.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.to_csv('aust_data_without_duplicates.csv', index=False)\n",
    "print('Duplicates have been removed, and the CSV file has been saved without duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78b2f2c1-3651-47bf-b162-ab3c6f6b2646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a6f71-4afb-418f-84f3-6cd7469ca5ab",
   "metadata": {},
   "source": [
    "Check if the locations are valid or not by passing through the code, this is to eliminate false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e903f1c6-b00f-48e9-9325-7ecab9d6fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/aust_data_without_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "490e8bda-844c-43c5-8229-23b899c1fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55970424-34d1-4e6c-8bd3-587f5f907dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_location_with_geonames(word):\n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    is_proper_noun = any(pos in ['NNP', 'NNPS'] for _, pos in tagged_words)\n",
    "    \n",
    "    if not is_proper_noun:\n",
    "        return False\n",
    "\n",
    "    # Check if the word is a location using Geonames\n",
    "    geolocator = Nominatim(user_agent=\"location_checker\",timeout=8)\n",
    "    try:\n",
    "        location = geolocator.geocode(word)\n",
    "        if location is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except GeocoderTimedOut:\n",
    "        return is_location_with_geonames(word)  # Retry the request if a timeout occurs\n",
    "    \n",
    "# Create a boolean mask to indicate whether each row is a valid location or not\n",
    "valid_locations_mask = df['GPE'].apply(is_location_with_geonames)\n",
    "\n",
    "# Keep only the rows with valid locations and drop the rest\n",
    "df = df[valid_locations_mask]\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('final_aust_gpe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e626788-84a9-43d2-8b0a-a71b41d33344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/aust_data/final_aust_gpe.csv')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7692580-44ba-4270-ae47-5242e27c63a8",
   "metadata": {},
   "source": [
    "Visualisation based on the data, let's consider doing it on the previous version of the data as removing duplicates has removed many valid GPE points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d72be899-31c9-4d0e-8fff-27749d5dfe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a5514f2-be0a-49c0-91b7-3d8fb7282059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "Warmambool_center = [-38.3720954,142.4777782]  \n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=Warmambool_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#2F539B\" \n",
    "\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 20  \n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = df[\"sum\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in df.iterrows():\n",
    "    location = row[\"GPE\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"sum\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_aust.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "730cc551-f1bd-4a0e-be57-b47165a8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "Warmambool_center = [-38.3720954,142.4777782]\n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=Warmambool_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#2F539B\" \n",
    "\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 5  \n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = df[\"sum\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in df.iterrows():\n",
    "    location = row[\"GPE\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"sum\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_aust.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ecbc6-17d3-4b52-b915-565ecc72a48e",
   "metadata": {},
   "source": [
    "Generating a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4460a8b-275f-4e88-aa7b-c9fa5687cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Warmambool_center = [-38.3720954,142.4777782]  # Example coordinates for South-West Australia\n",
    "\n",
    "# Create a map centered at the specific location with the desired zoom level and tiles\n",
    "heatmap_map = folium.Map(location=Warmambool_center, zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(df['Latitude'], df['Longitude'], df['sum']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_aust.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "118e9546-3a15-4e24-9bce-0fc5c567b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Example coordinates for South-West Australia\n",
    "Warmambool_center = [-38.3720954, 142.4777782]\n",
    "\n",
    "# Create a map centered at the specific location with the desired zoom level and tiles\n",
    "heatmap_map = folium.Map(location=Warmambool_center, zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(df['Latitude'], df['Longitude'], df['sum']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Create a custom legend using HTML and CSS\n",
    "legend_html = \"\"\"\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 10px; left: 10px; width: 220px; height: 130px; \n",
    "                 background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; z-index:1000;\">\n",
    "     <div style=\"padding: 10px; font-size: 14px; font-weight: bold;\">Legend</div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">0 - 0.4: Blue</div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">0.4 - 0.65: Green</div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">0.65 - 1: Red</div>\n",
    "     </div>\n",
    "     \"\"\"\n",
    "heatmap_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_with_legend.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a827c82-9c86-4b31-ae06-a99c6dd14b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Example coordinates for South-West Australia\n",
    "Warmambool_center = [-38.3720954, 142.4777782]\n",
    "\n",
    "# Create a map centered at the specific location with the desired zoom level and tiles\n",
    "heatmap_map = folium.Map(location=Warmambool_center, zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(df['Latitude'], df['Longitude'], df['sum']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Create a custom legend using HTML and CSS\n",
    "legend_html = \"\"\"\n",
    "<div style=\"position: fixed; \n",
    "            bottom: 10px; left: 10px; width: 220px; height: 130px; \n",
    "            background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; z-index:1000;\">\n",
    "    <div style=\"padding: 10px; font-size: 14px; font-weight: bold;\">Legend</div>\n",
    "    <div style=\"padding: 10px; font-size: 12px;\">\n",
    "        <div style=\"background-color: blue; width: 20px; height: 20px; display: inline-block;\"></div> 0 - 0.4: Blue\n",
    "    </div>\n",
    "    <div style=\"padding: 10px; font-size: 12px;\">\n",
    "        <div style=\"background-color: green; width: 20px; height: 20px; display: inline-block;\"></div> 0.4 - 0.65: Green\n",
    "    </div>\n",
    "    <div style=\"padding: 10px; font-size: 12px;\">\n",
    "        <div style=\"background-color: red; width: 20px; height: 20px; display: inline-block;\"></div> 0.65 - 1: Red\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "heatmap_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_with_legend_1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05286ca2-9415-4e28-b497-fbb632cf9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import FloatImage\n",
    "\n",
    "# Example coordinates for South-West Australia\n",
    "Warmambool_center = [-38.3720954, 142.4777782]\n",
    "\n",
    "# Create a map centered at the specific location with the desired zoom level and tiles\n",
    "heatmap_map = folium.Map(location=Warmambool_center, zoom_start=10, tiles='Stamen Terrain')\n",
    "\n",
    "# Create a list of tuples containing the Latitude, Longitude, and Occurrences for each data point\n",
    "heat_data = list(zip(df['Latitude'], df['Longitude'], df['sum']))\n",
    "\n",
    "# Add the HeatMap layer to the map with custom gradient and opacity\n",
    "HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, radius=15, blur=20).add_to(heatmap_map)\n",
    "\n",
    "# Create a custom legend using HTML and CSS\n",
    "legend_html = \"\"\"\n",
    "     <div style=\"position: fixed; \n",
    "                 bottom: 50px; left: 50px; width: 220px; \n",
    "                 background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; z-index:1000;\">\n",
    "     <div style=\"padding: 10px; font-size: 14px; font-weight: bold;\">Legend</div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">\n",
    "         <div style=\"background-color: blue; width: 20px; height: 20px; display: inline-block;\"></div> 0 - 0.4: Blue\n",
    "     </div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">\n",
    "         <div style=\"background-color: green; width: 20px; height: 20px; display: inline-block;\"></div> 0.4 - 0.65: Green\n",
    "     </div>\n",
    "     <div style=\"padding: 10px; font-size: 12px;\">\n",
    "         <div style=\"background-color: red; width: 20px; height: 20px; display: inline-block;\"></div> 0.65 - 1: Red\n",
    "     </div>\n",
    "     </div>\n",
    "     \"\"\"\n",
    "heatmap_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Define legend image URL (use a small transparent image)\n",
    "legend_img_url = 'https://via.placeholder.com/20x20.png'\n",
    "\n",
    "# Create a FloatImage with the legend image and position it on the map\n",
    "FloatImage(legend_img_url, bottom=50, left=50).add_to(heatmap_map)\n",
    "\n",
    "# Display the heatmap\n",
    "heatmap_map.save('heatmap_map_with_legend_3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f8f9c-2408-4347-8013-0071c99766fd",
   "metadata": {},
   "source": [
    "#Avoid the below cells code, it is for debugging each GPE in several ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "609923a0-d55c-4cbd-8717-67eb8e29f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Location': 'Sydney', 'Latitude': -33.8688197, 'Longitude': 151.2092955}]\n"
     ]
    }
   ],
   "source": [
    "geocoded_results = []\n",
    "location = 'Sydney'\n",
    "latitude, longitude = get_geolocation(location)\n",
    "geocoded_results.append({\"Location\": location, \"Latitude\": latitude, \"Longitude\": longitude})\n",
    "print(geocoded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2da1c70-892e-4df4-95d6-58ec18a5080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sydney is not a valid location.\n"
     ]
    }
   ],
   "source": [
    "def is_location_with_geonames(word):\n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    is_proper_noun = any(pos in ['NNP', 'NNPS'] for _, pos in tagged_words)\n",
    "    \n",
    "    if not is_proper_noun:\n",
    "        return False\n",
    "\n",
    "    # Check if the word is a location using Geonames\n",
    "    geolocator = Nominatim(user_agent=\"location_checker\", timeout=8)\n",
    "    try:\n",
    "        location = geolocator.geocode(word)\n",
    "        if location is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except GeocoderTimedOut:\n",
    "        return is_location_with_geonames(word)  # Retry the request if a timeout occurs\n",
    "\n",
    "# Test the function with a single GPE name\n",
    "sample_gpe_name = \"Sydney\"\n",
    "result = is_location_with_geonames(sample_gpe_name)\n",
    "\n",
    "if result:\n",
    "    print(f\"{sample_gpe_name} is a valid location.\")\n",
    "else:\n",
    "    print(f\"{sample_gpe_name} is not a valid location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c604e62-ec68-426b-8405-ce8bb25cd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the place is: Battery Lane, Port Fairy, Shire of Moyne, Victoria, 3284, Australia\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def reverse_geocode(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"reverse_geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    if location:\n",
    "        return location.address\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "latitude = -38.3920823  # Example latitude\n",
    "longitude = 142.2469658  # Example longitude\n",
    "\n",
    "place_name = reverse_geocode(latitude, longitude)\n",
    "print(f\"The name of the place is: {place_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2429f815-7343-43fb-b789-59478459aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the place is: Hamilton-Port Fairy Road, Port Fairy, Shire of Moyne, Victoria, 3284, Australia\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def reverse_geocode(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"reverse_geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    if location:\n",
    "        return location.address\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "latitude = -38.3806327  # Example latitude\n",
    "longitude = 142.2294039  # Example longitude\n",
    "\n",
    "place_name = reverse_geocode(latitude, longitude)\n",
    "print(f\"The name of the place is: {place_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f764f0a4-0f8b-420c-82f5-632abeeb917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city name for the location is: City not found\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_city_name(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    if location:\n",
    "        address = location.raw.get('address', {})\n",
    "        city = address.get('city', '')\n",
    "        if city:\n",
    "            return city\n",
    "        else:\n",
    "            return \"City not found\"\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "# Test the function with latitude and longitude data\n",
    "latitude = -42.0409059  # Example latitude\n",
    "longitude = 146.8087322  # Example longitude\n",
    "\n",
    "city_name = get_city_name(latitude, longitude)\n",
    "print(f\"The city name for the location is: {city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c1f7f-c98c-4457-8852-8fb183e38fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_cleaned_city_name(latitude, longitude):\n",
    "    # Your existing cleaning function here...\n",
    "    import re\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_cleaned_city_name(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    \n",
    "    if location:\n",
    "        city = location.raw.get('address', {}).get('city', '')\n",
    "        return clean_city_name(city)\n",
    "    else:\n",
    "        return \"City not found\"\n",
    "\n",
    "def clean_city_name(city_name):\n",
    "    remove_patterns = [\n",
    "        r'^City of ',\n",
    "        r'^Town of ',\n",
    "        r'^Village of ',\n",
    "        r'^Municipality of ',\n",
    "        r'^County of ',\n",
    "        r' City$', \n",
    "        r' Town$', \n",
    "        r' Village$', \n",
    "        r' Municipality$', \n",
    "        r' County$'\n",
    "    ]\n",
    "    \n",
    "    cleaned_name = city_name\n",
    "    for pattern in remove_patterns:\n",
    "        cleaned_name = re.sub(pattern, '', cleaned_name)\n",
    "    return cleaned_name.strip()\n",
    "\n",
    "# Test the function with latitude and longitude data\n",
    "latitude = 34.0522342  # Example latitude\n",
    "longitude = -118.2436849  # Example longitude\n",
    "\n",
    "cleaned_city_name = get_cleaned_city_name(latitude, longitude)\n",
    "print(f\"The cleaned city name for the location is: {cleaned_city_name}\")\n",
    "\n",
    "\n",
    "input_file = 'input_data.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Apply the cleaning function to each row and create a new column for cleaned city names\n",
    "df['Cleaned_City'] = df.apply(lambda row: get_cleaned_city_name(row['Latitude'], row['Longitude']), axis=1)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "output_file = 'output_data.csv'  # Replace with your desired output CSV file path\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1cb58a95-a16e-4d64-a864-9d5df6e6cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the place is: 274, McCoy Road, Bells, Apex, Chatham County, North Carolina, 27523, United States\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def reverse_geocode(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"reverse_geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    if location:\n",
    "        return location.address\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "# Test the function with latitude and longitude data\n",
    "latitude = 35.7595731  # Example latitude\n",
    "longitude = -79.0192996999999  # Example longitude\n",
    "\n",
    "\n",
    "place_name = reverse_geocode(latitude, longitude)\n",
    "print(f\"The name of the place is: {place_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78425ab7-c5b7-4217-b83c-b59ffaed0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned city name for the location is: Ohio\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_cleaned_city_name(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)  # Increase the radius\n",
    "    \n",
    "    if location:\n",
    "        address = location.raw.get('address', {})\n",
    "        \n",
    "        # Directly extract the location name if it exists in the address\n",
    "        location_name = address.get('city', '') or address.get('town', '') or address.get('village', '') or address.get('state', '') or address.get('country', '')\n",
    "        \n",
    "        if location_name:\n",
    "            return location_name\n",
    "        else:\n",
    "            return \"Location not found\"\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "latitude = 38.8199214  # Example latitude\n",
    "longitude = -83.357567  # Example longitude\n",
    "\n",
    "\n",
    "cleaned_city_name = get_cleaned_city_name(latitude, longitude)\n",
    "print(f\"The cleaned city name for the location is: {cleaned_city_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afb3383d-4729-4c40-9c11-6860d973dfd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_geolocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m geocoded_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSydney\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m latitude, longitude \u001b[38;5;241m=\u001b[39m \u001b[43mget_geolocation\u001b[49m(location)\n\u001b[1;32m      4\u001b[0m geocoded_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m: location, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: latitude, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m: longitude})\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(geocoded_results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_geolocation' is not defined"
     ]
    }
   ],
   "source": [
    "geocoded_results = []\n",
    "location = 'Sydney'\n",
    "latitude, longitude = get_geolocation(location)\n",
    "geocoded_results.append({\"Location\": location, \"Latitude\": latitude, \"Longitude\": longitude})\n",
    "print(geocoded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "707338d8-163a-4630-b8c4-fc7197dc30b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned city name for the location is: Kansas\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_cleaned_city_name(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"geocoder\")\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "    \n",
    "    if location:\n",
    "        location_name = None\n",
    "        \n",
    "        # Find the most relevant name from the address components\n",
    "        for key in ['city', 'town', 'state','country']:\n",
    "            if key in location.raw.get('address', {}):\n",
    "                location_name = location.raw['address'][key]\n",
    "                break\n",
    "        \n",
    "        if location_name:\n",
    "            cleaned_city_name = clean_city_name(location_name)\n",
    "            return cleaned_city_name\n",
    "        else:\n",
    "            return \"Location not found\"\n",
    "    else:\n",
    "        return \"Location not found\"\n",
    "\n",
    "# Test the function with latitude and longitude data\n",
    "latitude = 37.09024  # Example latitude\n",
    "longitude = -95.712891  # Example longitude\n",
    "\n",
    "cleaned_city_name = get_cleaned_city_name(latitude, longitude)\n",
    "print(f\"The cleaned city name for the location is: {cleaned_city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840a7ef-ec10-45ea-8e71-bff93694e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

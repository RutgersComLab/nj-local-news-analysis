{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b33809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from shapely import wkt\n",
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import random\n",
    "from spacy.lang.en.examples import sentences \n",
    "from geopy.distance import great_circle\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ade544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Location', 'Latitude', 'Longitude', 'Count'], dtype='object')\n",
      "(1123, 4)\n"
     ]
    }
   ],
   "source": [
    "#Small analysis and processing task \n",
    "njglobe_1 = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/newjerseyglobe_locations_1.csv')\n",
    "print(njglobe_1.columns)\n",
    "print(njglobe_1.shape)\n",
    "#print(len(fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c13819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 4)\n"
     ]
    }
   ],
   "source": [
    "njglobe_1 = njglobe_1.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "print(njglobe_1.shape)\n",
    "#So there are 23 rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a5ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 10\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Pick a specific color (e.g., red)\n",
    "color = \"#FF0000\"\n",
    "\n",
    "# Add circle markers for each location (dots) with the chosen color and labels\n",
    "for index, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    popup_text = f\"Location: {location}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=5, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"trial_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dbb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 10\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = njglobe_1[\"Count\"].max()\n",
    "\n",
    "# Function to map count values to marker sizes (adjust the scale factor as needed)\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 10  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Add circle markers for each location with varying size based on the count and labels\n",
    "for index, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"Count\"]\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    marker_size = get_marker_size(count)\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=\"#FF0000\", fill=True, fill_color=\"#FF0000\").add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"trial_3.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1caa37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the map center and zoom level as before\n",
    "map_center = [njglobe_1[\"Latitude\"].mean(), njglobe_1[\"Longitude\"].mean()]\n",
    "map_zoom = 5\n",
    "mymap = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# Set a single color for all the markers\n",
    "color = \"#FF0000\"  # Red color\n",
    "\n",
    "# Define a function to map count values to marker sizes\n",
    "def get_marker_size(count):\n",
    "    scale_factor = 40  # Adjust this value to control the marker size scaling\n",
    "    return 5 + (count / max_count) * scale_factor\n",
    "\n",
    "# Find the maximum count to use for scaling the marker size\n",
    "max_count = njglobe_1[\"Count\"].max()\n",
    "\n",
    "# Add circle markers with varying size based on the count\n",
    "for _, row in njglobe_1.iterrows():\n",
    "    location = row[\"Location\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    count = row[\"Count\"]\n",
    "    marker_size = get_marker_size(count)\n",
    "    popup_text = f\"Location: {location}\\nCount: {count}\"\n",
    "    folium.CircleMarker([latitude, longitude], popup=popup_text, radius=marker_size, color=color, fill=True, fill_color=color).add_to(mymap)\n",
    "\n",
    "# Saving the map\n",
    "mymap.save(\"scatter_plot_visualization_3.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "715d14e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235439, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "six_abc = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/news_articles_info_6abc.com.csv')\n",
    "six_abc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa405e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235439, 9)\n"
     ]
    }
   ],
   "source": [
    "six_abc = six_abc.dropna(subset=[\"gpe_latitude\", \"gpe_longitude\"])\n",
    "print(six_abc.shape)\n",
    "#no null data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4d1d8",
   "metadata": {},
   "source": [
    "#Grouping the location names based on the same latitude and longitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25f87cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'grouped_data_six_abc.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Group the data based on 'gpe_latitude' and 'gpe_longitude' and create a new column 'group_ID'\n",
    "six_abc['group_ID'] = six_abc.groupby(['gpe_latitude', 'gpe_longitude']).ngroup()\n",
    "\n",
    "# Sort the DataFrame based on the 'group_ID' column\n",
    "six_abc.sort_values(by='group_ID', inplace=True)\n",
    "\n",
    "# Reset the index after sorting (optional)\n",
    "six_abc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "six_abc.to_csv('grouped_data_six_abc.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'grouped_data_six_abc.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e984213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['news_url', 'outlet', 'outlet_latitude', 'outlet_longitude', 'gpe',\n",
      "       'gpe_latitude', 'gpe_longitude', 'gpe_county', 'gpe_occurrences',\n",
      "       'group_ID'],\n",
      "      dtype='object')\n",
      "(235439, 10)\n"
     ]
    }
   ],
   "source": [
    "#Small analysis and processing task \n",
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/grouped_data_six_abc.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "#print(len(fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dcd75",
   "metadata": {},
   "source": [
    "#Rename the gpe based on the common string for the same group_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ef0ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('group_ID')['gpe'].apply(list).reset_index()\n",
    "\n",
    "# Function to find the common pattern among a list of strings\n",
    "def find_common_pattern(strings):\n",
    "    if len(strings) == 0:\n",
    "        return None\n",
    "\n",
    "    # Initialize SequenceMatcher with the first string\n",
    "    seq_matcher = SequenceMatcher(None, strings[0], \"\")\n",
    "\n",
    "    # Iterate through the rest of the strings and find the longest common substring\n",
    "    common_substring = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        seq_matcher.set_seq2(s)\n",
    "        match = seq_matcher.find_longest_match(0, len(strings[0]), 0, len(s))\n",
    "        if match.size > 0:\n",
    "            common_substring = strings[0][match.a: match.a + match.size]\n",
    "            break\n",
    "\n",
    "    # Check if the pattern is valid and appears in all strings\n",
    "    if common_substring and all(common_substring in s for s in strings):\n",
    "        return common_substring\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Find the common pattern for each group and rename the location names accordingly\n",
    "for _, group in grouped_df.iterrows():\n",
    "    group_id = group['group_ID']\n",
    "    locations = group['gpe']\n",
    "\n",
    "    common_pattern = find_common_pattern(locations)\n",
    "\n",
    "    if common_pattern:\n",
    "        # Update the location names with the common pattern\n",
    "        df.loc[df['group_ID'] == group_id, 'gpe'] = common_pattern\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df.to_csv('modified_location_data_six_abc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58737d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235439, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa5980",
   "metadata": {},
   "source": [
    "#Delete duplicate values for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4fc5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/gayathri/Documents/nj-local-news-analysis/modified_location_data_six_abc.csv')\n",
    "# df.drop_duplicates(subset='gpe', keep='first', inplace=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36b460dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('no_duplicate_data_six_abc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f703f",
   "metadata": {},
   "source": [
    "#Evaluate if each gpe is a valid location"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

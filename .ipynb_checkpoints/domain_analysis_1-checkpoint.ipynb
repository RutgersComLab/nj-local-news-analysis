{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b861b53-1f5b-4b02-a80a-fa3deeac06b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gayathri/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/gayathri/myenv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from geopy.geocoders import GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderQuotaExceeded\n",
    "import time\n",
    "import re\n",
    "from geopy.geocoders import GoogleV3\n",
    "import time\n",
    "import requests\n",
    "from fuzzywuzzy import process\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4b2e68-5ad4-4860-804a-d73c81f1a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/Users/gayathri/Documents/GPE_intra_domain_cleaned/'\n",
    "final_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "\n",
    "api_key = 'AIzaSyBJ8P42fvaYv5pmqNdEqYEOJPENnm7eND0'\n",
    "geolocator = GoogleV3(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b915b447-9e66-4569-b02d-8fb1c52a82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_based_location(csv_data, file):\n",
    "    csv_data['group_ID'] = csv_data.groupby(['gpe_latitude', 'gpe_longitude']).ngroup()\n",
    "    csv_data.sort_values(by='group_ID', inplace=True)\n",
    "    csv_data.reset_index(drop=True, inplace=True)\n",
    "    save_file = \"grouped_data_\" + file + \".csv\"  \n",
    "    output_file_path = os.path.join(output_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"file saved after grouping \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "def find_most_relevant_gpe(group_df):\n",
    "    gpe_variations = group_df['gpe'].tolist()\n",
    "    best_match, _ = process.extractOne(group_df['gpe'].iloc[0], gpe_variations, scorer=fuzz.ratio)\n",
    "    group_df['gpe_new'] = best_match \n",
    "    return group_df  \n",
    "\n",
    "def format_gpe(csv_data, file):\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: re.sub(r\"'s$\", \"\", x))\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: x.title())\n",
    "    csv_data['gpe'] = csv_data['gpe'].apply(lambda x: x.replace('@', ''))\n",
    "    csv_data = csv_data.loc[(csv_data['gpe_latitude'] != 0) & (csv_data['gpe_longitude'] != 0)]\n",
    "    print(\"done with setting gpename\")\n",
    "    csv_data = csv_data.groupby('group_ID').apply(find_most_relevant_gpe).reset_index(level=0, drop=True)\n",
    "    print(\"done with formatting\")\n",
    "    save_file = \"formatted_data_\" + file + \".csv\"  \n",
    "    output_file_path = os.path.join(output_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"File saved after formatting: \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "def sum_and_no_duplicates(csv_data,file):\n",
    "    csv_data['gpe_sum'] = csv_data.groupby(['gpe_new', 'group_ID'])['gpe_occurrences'].transform('sum')\n",
    "    csv_data = csv_data.drop_duplicates(subset=['gpe_new'], keep='first')\n",
    "    save_file = \"no_duplicates_\" + file + \".csv\" \n",
    "    output_file_path = os.path.join(final_folder, save_file)\n",
    "    csv_data.to_csv(output_file_path, index=False)\n",
    "    print(\"file saved after sum_and_no_duplicates \", save_file)\n",
    "    return csv_data\n",
    "\n",
    "# def is_location_with_spacy(word):\n",
    "#     doc = nlp(word)\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ == \"GPE\":\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def validate_gpe_with_spacy(csv_data, file):\n",
    "#     # Create a new column 'Is_Valid' in the DataFrame to store validation results\n",
    "#     csv_data['Is_Valid'] = csv_data['gpe_new'].apply(lambda x: is_location_with_spacy(x))\n",
    "#     # Convert 'Is_Valid' to 1 for valid and 0 for invalid\n",
    "#     csv_data['Is_Valid'] = csv_data['Is_Valid'].astype(int)\n",
    "#     save_file = \"valid_gpe_\" + file + \".csv\"\n",
    "#     output_file_path = os.path.join(output_folder, save_file)\n",
    "#     csv_data.to_csv(output_file_path, index=False)\n",
    "#     print(\"File saved after validate_gpe: \", save_file)\n",
    "#     return csv_data\n",
    "\n",
    "def create_heatmap(csv_data, file):\n",
    "    max_occurrences_row = csv_data.loc[csv_data['gpe_sum'].idxmax()]\n",
    "    highest_occurrences_latitude = max_occurrences_row['gpe_latitude']\n",
    "    highest_occurrences_longitude = max_occurrences_row['gpe_longitude']\n",
    "    highest_occurrence = [highest_occurrences_latitude, highest_occurrences_longitude]\n",
    "    print(highest_occurrence)\n",
    "    highest_gpe = max_occurrences_row['gpe_new']\n",
    "    print(highest_gpe)\n",
    "    heatmap_map = folium.Map(location=highest_occurrence, zoom_start=10, tiles='OpenStreetMap', max_zoom=5)\n",
    "    heat_data = list(zip(csv_data['gpe_latitude'], csv_data['gpe_longitude'], csv_data['gpe_sum']))\n",
    "    HeatMap(heat_data, gradient={0.4: 'blue', 0.65: 'green', 1: 'red'}, opacity=0.7, min_opacity=0.5, radius=15, blur=20).add_to(heatmap_map)\n",
    "    disable_interactivity_js = \"\"\"\n",
    "    function disableInteractivity() {\n",
    "        var map = document.getElementById('map');\n",
    "        map.style['pointer-events'] = 'none';\n",
    "    }\n",
    "    disableInteractivity();\n",
    "    \"\"\"\n",
    "    \n",
    "    folium.Element(disable_interactivity_js).add_to(heatmap_map)\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 20px; width: 180px; \n",
    "                background-color: rgba(255, 255, 255, 0.8); border-radius: 5px; z-index: 1000;\">\n",
    "        <div style=\"text-align: center; padding: 10px; font-size: 14px; font-weight: bold;\">Relative frequency of location count legend (0 to 1 Scale)</div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: blue; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0 - 0.4: Blue</span>\n",
    "        </div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: green; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0.4 - 0.65: Green</span>\n",
    "        </div>\n",
    "        <div style=\"padding: 10px; font-size: 12px;\">\n",
    "            <div style=\"background-color: red; width: 20px; height: 20px; display: inline-block; vertical-align: middle;\"></div>\n",
    "            <span style=\"vertical-align: middle;\">0.65 - 1: Red</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    save_file = \"heatmap_\" + file\n",
    "    heatmap_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "    #csv_data.to_csv(output_file_path, index=False)\n",
    "    heatmap_map.save(output_folder + save_file + '.html')\n",
    "    print(\"Heatmap will be saved to:\", output_folder + save_file + '.html')\n",
    "\n",
    "    return heatmap_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a079a39-c8cb-4bee-b39d-5f1d042f6599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved after grouping  grouped_data_tcnjsignal.csv\n",
      "done with setting gpename\n",
      "done with formatting\n",
      "File saved after formatting:  formatted_data_tcnjsignal.csv\n",
      "file saved after sum_and_no_duplicates  no_duplicates_tcnjsignal.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path where your CSV files are located\n",
    "# folder_path = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "\n",
    "csv_data = pd.read_csv('/Users/gayathri/Documents/GPE_intra_domain_cleaned/news_articles_info_www.tcnjsignal.net.csv')\n",
    "file = 'tcnjsignal'\n",
    "invalid_places = \"invalid_places\"+file\n",
    "csv_data = grouping_based_location(csv_data,file)\n",
    "csv_data = format_gpe(csv_data,file)\n",
    "csv_data = sum_and_no_duplicates(csv_data,file)\n",
    "#csv_data_heaptmap = create_heatmap(csv_data,file)\n",
    "#csv_data = validate_gpe_with_spacy(csv_data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b49c15b-7e04-4118-a324-af8950371e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CSV files in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned: 118\n",
      "Total number of CSV files in the folder /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data: 119\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_csv_files(folder_path):\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "    return len(csv_files)\n",
    "\n",
    "# Replace 'your_folder_path' with the actual path to your folder\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "results_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "\n",
    "try:\n",
    "    total_csv_files = count_csv_files(input_folder)\n",
    "    print(f'Total number of CSV files in {input_folder}: {total_csv_files}')\n",
    "    total_csv_files = count_csv_files(results_folder)\n",
    "    print(f'Total number of CSV files in the folder {results_folder}: {total_csv_files}')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {folder_path} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e7c9a1-ec59-4eb5-a74a-af4247525620",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m file \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the percentiles for 'gpe_sum'\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m percentiles \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39munique(csv_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpe_sum\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile([\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m]))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Define bins with 0 as the start, percentiles as intermediate values, and the maximum value plus a small epsilon as the end\u001b[39;00m\n\u001b[1;32m     19\u001b[0m bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m percentiles \u001b[38;5;241m+\u001b[39m [csv_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpe_sum\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.01\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder path where your CSV files are located\n",
    "no_duplicates_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "data_with_quadrants_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/Data_With_Quadrants'\n",
    "\n",
    "# Iterate through each CSV file in the folder\n",
    "for filename in os.listdir(no_duplicates_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(no_duplicates_folder, filename)\n",
    "        # Read the CSV file\n",
    "        csv_data = pd.read_csv(file_path)\n",
    "        parts = filename.split('_')\n",
    "        file = parts[-1].split('.')[0]\n",
    "        # Calculate the percentiles for 'gpe_sum'\n",
    "        percentiles = np.unique(csv_data['gpe_sum'].quantile([0.25, 0.5, 0.75])).tolist()\n",
    "        # Define bins with 0 as the start, percentiles as intermediate values, and the maximum value plus a small epsilon as the end\n",
    "        bins = [0] + percentiles + [csv_data['gpe_sum'].max() + 0.01]\n",
    "        # Assign quadrants using these bins\n",
    "        csv_data['Quadrant'] = pd.cut(csv_data['gpe_sum'], bins=bins, labels=['Quadrant 1', 'Quadrant 2', 'Quadrant 3', 'Quadrant 4'], include_lowest=True)\n",
    "        # Sort the dataset based on the 'Quadrant' column\n",
    "        csv_data.sort_values(by=['Quadrant'], inplace=True)\n",
    "        save_file = \"quadrants_\" + file + \".csv\" \n",
    "        output_file_path = os.path.join(data_with_quadrants_folder, save_file)\n",
    "        csv_data.to_csv(output_file_path, index=False)\n",
    "        print(\"file saved after adding the quadrants info\", save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b86904c5-e61a-48c2-a0fe-9b8d6ca7784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the cells below this are for debugging purpose only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd0913a-d451-4752-a175-69e565bdf73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newjersey.news12.com.csv\n",
      "www.jerseyshoreonline.com.csv\n",
      "920thejersey.com.csv\n",
      "www.hobokenhorse.com.csv\n",
      "www.mycentraljersey.com.csv\n",
      "eccobserverdotcom.wordpress.com.csv\n",
      "wpgtalkradio.com.csv\n",
      "www.indiaabroad.com.csv\n",
      "www.wlvt.org.csv\n",
      "www.jerseyvoices.com.csv\n",
      "medium.com.csv\n",
      "www.stocktonargo.com.csv\n",
      "princetoninfo.com.csv\n",
      "www.951wayv.com.csv\n",
      "www.rahwayrising.com.csv\n",
      "www.dailyprincetonian.com.csv\n",
      "wpst.com.csv\n",
      "radio.rutgers.edu.csv\n",
      "www.westmilfordmessenger.com.csv\n",
      "www.shorelocalnews.com.csv\n",
      "pocono967.com.csv\n",
      "www1.nyc.gov.csv\n",
      "rennamedia.com.csv\n",
      "southjerseyobserver.com.csv\n",
      "newstalk990.com.csv\n",
      "www.anointedonline.net.csv\n",
      "thepakistaninewspaper.com.csv\n",
      "www.irishcentral.com.csv\n",
      "www.dirt-mag.com.csv\n",
      "www.civicstory.org.csv\n",
      "phl17.com.csv\n",
      "montclairdispatch.com.csv\n",
      "www.divyabhaskar.co.in.csv\n",
      "www.wnyc.org.csv\n",
      "www.mcccvoice.org.csv\n",
      "njrevolutionradio.com.csv\n",
      "www.pressofatlanticcity.com.csv\n",
      "www.tristatevoice.com.csv\n",
      "literock969.com.csv\n",
      "www.urbanagendamagazine.com.csv\n",
      "943thepoint.com.csv\n",
      "nepszava.u.csv\n",
      "www.lavocedinewyork.com.csv\n",
      "www.southjerseybiz.net.csv\n",
      "www.the-gazette-newspaper.com.csv\n",
      "downbeachbuzz.com.csv\n",
      "www.theyeshivaworld.com.csv\n",
      "www.newsindiatimes.com.csv\n",
      "newjerseyglobe.com.csv\n",
      "www.spartaindependent.com.csv\n",
      "www.law.com.csv\n",
      "1057thehawk.com.csv\n",
      "sojo1049.com.csv\n",
      "www.my9nj.com.csv\n",
      "www.posteaglenewspaper.com.csv\n",
      "gujaratdarpan.com.csv\n",
      "mycommunitysource.com.csv\n",
      "wurdradio.com.csv\n",
      "njbmagazine.com.csv\n",
      "987thecoast.com.csv\n",
      "www.recordonline.com.csv\n",
      "newtownpress.com.csv\n",
      "www.ahherald.com.csv\n",
      "www.wliw.org.csv\n",
      "njjewishnews.timesofisrael.com.csv\n",
      "latribunanj.com.csv\n",
      "www.phillyvoice.com.csv\n",
      "6abc.com.csv\n",
      "thepositivecommunity.com.csv\n",
      "www.wqxr.org.csv\n",
      "countywatchers.wordpress.com.csv\n",
      "www.tygodnikplus.com.csv\n",
      "essexnewsdaily.com.csv\n",
      "unionnewsdaily.com.csv\n",
      "hellenicnews.com.csv\n",
      "www.princetonmagazine.com.csv\n",
      "www.hammontongazette.com.csv\n",
      "www.roi-nj.com.csv\n",
      "fduequinox.wordpress.com.csv\n",
      "wsus1023.iheart.com.csv\n",
      "www.ntd.t.csv\n",
      "www.tcnjsignal.net.csv\n",
      "www.rcan.org.csv\n",
      "savejersey.com.csv\n",
      "www.courierpostonline.com.csv\n",
      "www.1077thebronc.com.csv\n",
      "brick.shorebeat.com.csv\n",
      "cccvoice.wordpress.com.csv\n",
      "philadelphia.cbslocal.com.csv\n",
      "www.wsou.net.csv\n",
      "www.visionsnewspaper.com.csv\n",
      "www.telemundo47.com.csv\n",
      "www.koreadailyus.com.csv\n",
      "www.theridernews.com.csv\n",
      "www.thedrewacorn.com.csv\n",
      "pinebarrenstribune.com.csv\n",
      "www.wvlt.com.csv\n",
      "www.townshipjournal.com.csv\n",
      "www.wwfm.org.csv\n",
      "www.themontynews.org.csv\n",
      "thecoaster.net.csv\n",
      "www.northjersey.com.csv\n",
      "wbjb.org.csv\n",
      "www.njpen.com.csv\n",
      "dailyvoice.com.csv\n",
      "forward.com.csv\n",
      "newjerseybuzz.com.csv\n",
      "www.towntopics.com.csv\n",
      "www.thecore.fm.csv\n",
      "www.classicoldieswmid.com.csv\n",
      "www.tapinto.net.csv\n",
      "am970theanswer.com.csv\n",
      "brigantinenow.com.csv\n",
      "thenjsentinel.com.csv\n",
      "baristanet.com.csv\n",
      "www.ocsentinel.com.csv\n",
      "www.theobserver.com.csv\n",
      "wputimes.wordpress.com.csv\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_file_name(file_name, prefix):\n",
    "    if file_name.startswith(prefix):\n",
    "        file_name = file_name[len(prefix):]\n",
    "    return file_name\n",
    "\n",
    "def get_files_without_prefix(folder_path, prefix):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    input_files = [process_file_name(file, prefix) for file in files]\n",
    "    return input_files\n",
    "\n",
    "# Replace 'your_folder_path' and 'news_articles_info_' with the actual folder path and prefix\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/GPE_intra_domain_cleaned'\n",
    "prefix_to_remove = 'news_articles_info_'\n",
    "\n",
    "try:\n",
    "    input_files = get_files_without_prefix(input_folder, prefix_to_remove)\n",
    "    # print(f'File names without \"{prefix_to_remove}\" in {input_folder}:')\n",
    "    for file_name in input_files:\n",
    "        print(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {input_folder} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "print(len(input_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07230df9-84d3-4622-89ce-e2c601419a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names without \"no_duplicates_\" in /Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data:\n",
      "brick.shorebeat.csv\n",
      "montynews.csv\n",
      "njjewishnews.timesofisrael.csv\n",
      "wpgtalkradio.csv\n",
      "www.recordonline.csv\n",
      "www.951wayv.csv\n",
      "wsou.csv\n",
      "dailyvoice.csv\n",
      "njrevolutionradio.csv\n",
      "www.roi-nj.csv\n",
      "divyabhaskar.csv\n",
      "literock969.csv\n",
      "cccvoice.wordpress.csv\n",
      "www.posteaglenewspaper.csv\n",
      "www.njpen.csv\n",
      "am970theanswer.csv\n",
      "civicstory.csv\n",
      "phl17.csv\n",
      "philadelphia.cbslocal.csv\n",
      "www.telemundo47.csv\n",
      "www.newsindiatimes.csv\n",
      "www.koreadailyus.csv\n",
      "www.stocktonargo.csv\n",
      "wlvt.csv\n",
      "newjerseybuzz.csv\n",
      "newtownpress.csv\n",
      "www.jerseyvoices.csv\n",
      "www.hobokenhorse.csv\n",
      "hellenicnews.csv\n",
      "1057thehawk.csv\n",
      "thepositivecommunity.csv\n",
      "thecoaster.csv\n",
      "tcnjsignal.csv\n",
      "www.law.csv\n",
      "brigantinenow.csv\n",
      "anointedonline.csv\n",
      "pinebarrenstribune.csv\n",
      "www.hammontongazette.csv\n",
      "savejersey.csv\n",
      "rcan.csv\n",
      "www.my9nj.csv\n",
      "forward.csv\n",
      "pocono967.csv\n",
      "wbjb.csv\n",
      "theobserver.csv\n",
      "www.princetonmagazine.csv\n",
      "radio.csv\n",
      "www.theridernews.csv\n",
      "mycommunitysource.csv\n",
      "fduequinox.wordpress.csv\n",
      "medium.csv\n",
      "thepakistaninewspaper.csv\n",
      "wliw.csv\n",
      "wwfm.csv\n",
      "sojo1049.csv\n",
      "www.northjersey.csv\n",
      "6abc.csv\n",
      "www.mycentraljersey.csv\n",
      "eccobserverdotcom.wordpress.csv\n",
      "www.townshipjournal.csv\n",
      "www.the-gazette-newspaper.csv\n",
      "www.1077thebronc.csv\n",
      "www.rahwayrising.csv\n",
      "nepszava.csv\n",
      "www.lavocedinewyork.csv\n",
      "www.indiaabroad.csv\n",
      "www.towntopics.csv\n",
      "rennamedia.csv\n",
      "www.tygodnikplus.csv\n",
      "www.shorelocalnews.csv\n",
      "www.visionsnewspaper.csv\n",
      "www.tristatevoice.csv\n",
      "baristanet.csv\n",
      "www.theyeshivaworld.csv\n",
      "wputimes.wordpress.csv\n",
      "newjerseyglobe.csv\n",
      "www.classicoldieswmid.csv\n",
      "www.pressofatlanticcity.csv\n",
      "www.jerseyshoreonline.csv\n",
      "newstalk990.csv\n",
      "latribunanj.csv\n",
      "www.wvlt.csv\n",
      "njbmagazine.csv\n",
      "920thejersey.csv\n",
      "wurdradio.csv\n",
      "www.urbanagendamagazine.csv\n",
      "www.dirt-mag.csv\n",
      "www.phillyvoice.csv\n",
      "wnyc.csv\n",
      "www.thedrewacorn.csv\n",
      "thenjsentinel.csv\n",
      "www.ocsentinel.csv\n",
      "montclairdispatch.csv\n",
      "987thecoast.csv\n",
      "www.spartaindependent.csv\n",
      "mcccvoice.csv\n",
      "www.courierpostonline.csv\n",
      "thecore.csv\n",
      "downbeachbuzz.csv\n",
      "www.ahherald.csv\n",
      "nyc.csv\n",
      "ntd.csv\n",
      "www.irishcentral.csv\n",
      "wpst.csv\n",
      "943thepoint.csv\n",
      "southjerseyobserver.csv\n",
      "wqxr.csv\n",
      "unionnewsdaily.csv\n",
      "www.westmilfordmessenger.csv\n",
      "essexnewsdaily.csv\n",
      "wsus1023.iheart.csv\n",
      "princetoninfo.csv\n",
      "tapinto.csv\n",
      "gujaratdarpan.csv\n",
      "newjersey.news12.csv\n",
      "www.dailyprincetonian.csv\n",
      "www.theobserver.csv\n",
      "southjerseybiz.csv\n",
      "countywatchers.wordpress.csv\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_file_name(file_name, prefix):\n",
    "    if file_name.startswith(prefix):\n",
    "        file_name = file_name[len(prefix):]\n",
    "    return file_name\n",
    "\n",
    "def get_files_without_prefix(folder_path, prefix):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    processed_files = [process_file_name(file, prefix) for file in files]\n",
    "    return processed_files\n",
    "\n",
    "# Replace 'your_folder_path' and 'news_articles_info_' with the actual folder path and prefix\n",
    "input_folder = '/Users/gayathri/Library/CloudStorage/Box-Box/Local News Data/No_Duplicates_Data'\n",
    "prefix_to_remove = 'no_duplicates_'\n",
    "\n",
    "try:\n",
    "    processed_files = get_files_without_prefix(input_folder, prefix_to_remove)\n",
    "    print(f'File names without \"{prefix_to_remove}\" in {input_folder}:')\n",
    "    for file_name in processed_files:\n",
    "        print(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f'The specified folder path {input_folder} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "print(len(processed_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cd7887-f9e7-466a-a7a1-f09b443aafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newjersey.news12.com.csv', 'www.jerseyshoreonline.com.csv', '920thejersey.com.csv', 'www.hobokenhorse.com.csv', 'www.mycentraljersey.com.csv', 'eccobserverdotcom.wordpress.com.csv', 'wpgtalkradio.com.csv', 'www.indiaabroad.com.csv', 'www.wlvt.org.csv', 'www.jerseyvoices.com.csv', 'medium.com.csv', 'www.stocktonargo.com.csv', 'princetoninfo.com.csv', 'www.951wayv.com.csv', 'www.rahwayrising.com.csv', 'www.dailyprincetonian.com.csv', 'wpst.com.csv', 'radio.rutgers.edu.csv', 'www.westmilfordmessenger.com.csv', 'www.shorelocalnews.com.csv', 'pocono967.com.csv', 'www1.nyc.gov.csv', 'rennamedia.com.csv', 'southjerseyobserver.com.csv', 'newstalk990.com.csv', 'www.anointedonline.net.csv', 'thepakistaninewspaper.com.csv', 'www.irishcentral.com.csv', 'www.dirt-mag.com.csv', 'www.civicstory.org.csv', 'phl17.com.csv', 'montclairdispatch.com.csv', 'www.divyabhaskar.co.in.csv', 'www.wnyc.org.csv', 'www.mcccvoice.org.csv', 'njrevolutionradio.com.csv', 'www.pressofatlanticcity.com.csv', 'www.tristatevoice.com.csv', 'literock969.com.csv', 'www.urbanagendamagazine.com.csv', '943thepoint.com.csv', 'nepszava.u.csv', 'www.lavocedinewyork.com.csv', 'www.southjerseybiz.net.csv', 'www.the-gazette-newspaper.com.csv', 'downbeachbuzz.com.csv', 'www.theyeshivaworld.com.csv', 'www.newsindiatimes.com.csv', 'newjerseyglobe.com.csv', 'www.spartaindependent.com.csv', 'www.law.com.csv', '1057thehawk.com.csv', 'sojo1049.com.csv', 'www.my9nj.com.csv', 'www.posteaglenewspaper.com.csv', 'gujaratdarpan.com.csv', 'mycommunitysource.com.csv', 'wurdradio.com.csv', 'njbmagazine.com.csv', '987thecoast.com.csv', 'www.recordonline.com.csv', 'newtownpress.com.csv', 'www.ahherald.com.csv', 'www.wliw.org.csv', 'njjewishnews.timesofisrael.com.csv', 'latribunanj.com.csv', 'www.phillyvoice.com.csv', '6abc.com.csv', 'thepositivecommunity.com.csv', 'www.wqxr.org.csv', 'countywatchers.wordpress.com.csv', 'www.tygodnikplus.com.csv', 'essexnewsdaily.com.csv', 'unionnewsdaily.com.csv', 'hellenicnews.com.csv', 'www.princetonmagazine.com.csv', 'www.hammontongazette.com.csv', 'www.roi-nj.com.csv', 'fduequinox.wordpress.com.csv', 'wsus1023.iheart.com.csv', 'www.ntd.t.csv', 'www.tcnjsignal.net.csv', 'www.rcan.org.csv', 'savejersey.com.csv', 'www.courierpostonline.com.csv', 'www.1077thebronc.com.csv', 'brick.shorebeat.com.csv', 'cccvoice.wordpress.com.csv', 'philadelphia.cbslocal.com.csv', 'www.wsou.net.csv', 'www.visionsnewspaper.com.csv', 'www.telemundo47.com.csv', 'www.koreadailyus.com.csv', 'www.theridernews.com.csv', 'www.thedrewacorn.com.csv', 'pinebarrenstribune.com.csv', 'www.wvlt.com.csv', 'www.townshipjournal.com.csv', 'www.wwfm.org.csv', 'www.themontynews.org.csv', 'thecoaster.net.csv', 'www.northjersey.com.csv', 'wbjb.org.csv', 'www.njpen.com.csv', 'dailyvoice.com.csv', 'forward.com.csv', 'newjerseybuzz.com.csv', 'www.towntopics.com.csv', 'www.thecore.fm.csv', 'www.classicoldieswmid.com.csv', 'www.tapinto.net.csv', 'am970theanswer.com.csv', 'brigantinenow.com.csv', 'thenjsentinel.com.csv', 'baristanet.com.csv', 'www.ocsentinel.com.csv', 'www.theobserver.com.csv', 'wputimes.wordpress.com.csv']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "result = [name for name in input_files if name not in processed_files]\n",
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0280250c-e073-4557-8e6d-0a9a6f012705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result list to a CSV file\n",
    "output_file_path = 'processed_2.csv'\n",
    "\n",
    "with open(output_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Names in processed_files'])\n",
    "    for name in processed_files:\n",
    "        csv_writer.writerow([name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a82a898-c01c-4453-8559-f646007b5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result list to a CSV file\n",
    "output_file_path = 'input.csv'\n",
    "\n",
    "with open(output_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Names in input_files'])\n",
    "    for name in input_files:\n",
    "        csv_writer.writerow([name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e72bcbb3-0362-4bcb-854b-581a97654624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to /Users/gayathri/Documents/input.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_com(cell_value):\n",
    "    \"\"\"\n",
    "    Remove '.com' from the cell value.\n",
    "    \"\"\"\n",
    "    return cell_value.replace('.com', '')\n",
    "\n",
    "def process_csv_inplace(file_path):\n",
    "    \"\"\"\n",
    "    Process a CSV file in-place, removing '.com' from the 'Names' column.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Assuming the column you want to process is named 'Names'\n",
    "    df['Names in input_files'] = df['Names in input_files'].apply(remove_com)\n",
    "\n",
    "    # Write the updated DataFrame back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# Replace 'your_file.csv' with your actual file path\n",
    "csv_file_path = '/Users/gayathri/Documents/input.csv'\n",
    "\n",
    "process_csv_inplace(csv_file_path)\n",
    "print(f'Processed data saved to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49e77ee-21d4-470b-9603-2e488869b2df",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gayathri/Documents/input.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m file2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/gayathri/Documents/processed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNames\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m \u001b[43mfind_missing_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile2_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing values in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile1_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that are not in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile2_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m missing_values:\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mfind_missing_values\u001b[0;34m(file1, file2, column_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mFind the missing values in column_name that are present in file1 and not in file2.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV files into DataFrames\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file2)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Get the values in column_name that are in file1 but not in file2\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gayathri/Documents/input.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_missing_values(file1, file2, column_name):\n",
    "    \"\"\"\n",
    "    Find the missing values in column_name that are present in file1 and not in file2.\n",
    "    \"\"\"\n",
    "    # Read the CSV files into DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Get the values in column_name that are in file1 but not in file2\n",
    "    missing_values = set(df1[column_name]) - set(df2[column_name])\n",
    "\n",
    "    return list(missing_values)\n",
    "\n",
    "# Replace 'file1.csv', 'file2.csv', and 'column_name' with your actual file paths and column name\n",
    "file1_path = '/Users/gayathri/Documents/input.csv'\n",
    "file2_path = '/Users/gayathri/Documents/processed.csv'\n",
    "column_name = 'Names'\n",
    "\n",
    "missing_values = find_missing_values(file1_path, file2_path, column_name)\n",
    "\n",
    "print(f'Missing values in {file1_path} that are not in {file2_path}:')\n",
    "for value in missing_values:\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e51c7-5adf-4352-8904-98d524daa7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
